{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSGF VS PeptideAtlas Reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reqired files (put under same folder):\n",
    "###### 1 all_usi.txt : all peptide atlas usi found\n",
    "###### 2 MassIVE-KB_HPP_proteins.tsv: all MassIVE proteins\n",
    "###### 3 all ambituity files on same directory eg. MSV000096271\n",
    "###### 4 fasta_file = 'uniprotkb_human_proteome_UP000005640_with_isoforms_2024-10-08.fasta'\n",
    "###### 5 PA_observations.csv peptide atlas all peptides\n",
    "###### 6 dictionary file Human_proteome_dictionary_I_replaced_by_L.pkl with I subsituted by L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_msgf = 'MSV000086793'\n",
    "dataset_pa = 'MSV000086793'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge ambiguity files --> ambiguity_merged.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the TSV files\n",
    "directory = 'D:/ccms/Protein_id_reanalysis/Evaluate_reanalysis/'+dataset_msgf\n",
    "\n",
    "# Output file path\n",
    "output_file = os.path.join(directory, 'ambiguity_merged.tsv')\n",
    "\n",
    "# Archive directory\n",
    "archive_directory = os.path.join(directory, 'ambiguity_archive')\n",
    "\n",
    "# Create the archive directory if it doesn't exist\n",
    "if not os.path.exists(archive_directory):\n",
    "    os.makedirs(archive_directory)\n",
    "\n",
    "# Remove the output file if it already exists\n",
    "# if os.path.exists(output_file):\n",
    "#     os.remove(output_file)\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('MSGF-PLUS-AMBIGUITY') and filename.endswith('.tsv'):\n",
    "        print(f'Processing file: {filename}')\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, sep='\\t', low_memory=False)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f'Skipping empty file: {filename}')\n",
    "            continue\n",
    "        # Append to the output file\n",
    "        df.to_csv(output_file, sep='\\t', index=False, mode='a', header=not os.path.exists(output_file))\n",
    "        # Move the processed file to the archive directory\n",
    "        shutil.move(filepath, os.path.join(archive_directory, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ambiguity_merged.tsv --> filtered.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets in chunks\n",
    "chunksize = 10000\n",
    "proteins_to_skip_file = 'MassIVE-KB_HPP_proteins.tsv'\n",
    "with open(proteins_to_skip_file, 'r') as file:\n",
    "    massive_kb_proteins = set(line.strip() for line in file)\n",
    "\n",
    "filtered_chunks = []\n",
    "for chunk in pd.read_csv('ambiguity_merged.tsv', sep='\\t', low_memory=False, error_bad_lines=False, chunksize=chunksize):\n",
    "    # Process the 'opt_global_TopCanonicalProtein' column to extract the protein ID\n",
    "\n",
    "    chunk['opt_global_TopCanonicalProtein'] = chunk['opt_global_TopCanonicalProtein'].apply(\n",
    "        lambda x: x.split('|')[1] if isinstance(x, str) and '|' in x else x\n",
    "    )\n",
    "    # Filter the chunk\n",
    "    filtered_chunk = chunk[~chunk['opt_global_TopCanonicalProtein'].isin(massive_kb_proteins)]\n",
    "    filtered_chunks.append(filtered_chunk)\n",
    "\n",
    "# Concatenate all filtered chunks\n",
    "filtered_reanalysis_df = pd.concat(filtered_chunks)\n",
    "\n",
    "# Save the filtered dataframe to a new file\n",
    "filtered_reanalysis_df.to_csv('filtered.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spectrum level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all_usi.txt + filtered.tsv --> example_reanalysis_spectrum.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 rows (0.36%). Estimated time left: 0h 0m 21s\n",
      "Processed 2000 rows (0.71%). Estimated time left: 0h 0m 16s\n",
      "Processed 3000 rows (1.07%). Estimated time left: 0h 0m 17s\n",
      "Processed 4000 rows (1.43%). Estimated time left: 0h 0m 17s\n",
      "Processed 5000 rows (1.79%). Estimated time left: 0h 0m 16s\n",
      "Processed 6000 rows (2.14%). Estimated time left: 0h 0m 15s\n",
      "Processed 7000 rows (2.50%). Estimated time left: 0h 0m 14s\n",
      "Processed 8000 rows (2.86%). Estimated time left: 0h 0m 14s\n",
      "Processed 9000 rows (3.21%). Estimated time left: 0h 0m 13s\n",
      "Processed 10000 rows (3.57%). Estimated time left: 0h 0m 13s\n",
      "Processed 11000 rows (3.93%). Estimated time left: 0h 0m 13s\n",
      "Processed 12000 rows (4.29%). Estimated time left: 0h 0m 13s\n",
      "Processed 13000 rows (4.64%). Estimated time left: 0h 0m 14s\n",
      "Processed 14000 rows (5.00%). Estimated time left: 0h 0m 13s\n",
      "Processed 15000 rows (5.36%). Estimated time left: 0h 0m 13s\n",
      "Processed 16000 rows (5.71%). Estimated time left: 0h 0m 13s\n",
      "Processed 17000 rows (6.07%). Estimated time left: 0h 0m 13s\n",
      "Processed 18000 rows (6.43%). Estimated time left: 0h 0m 13s\n",
      "Processed 19000 rows (6.79%). Estimated time left: 0h 0m 13s\n",
      "Processed 20000 rows (7.14%). Estimated time left: 0h 0m 13s\n",
      "Processed 21000 rows (7.50%). Estimated time left: 0h 0m 13s\n",
      "Processed 22000 rows (7.86%). Estimated time left: 0h 0m 13s\n",
      "Processed 23000 rows (8.21%). Estimated time left: 0h 0m 13s\n",
      "Processed 24000 rows (8.57%). Estimated time left: 0h 0m 13s\n",
      "Processed 25000 rows (8.93%). Estimated time left: 0h 0m 13s\n",
      "Processed 26000 rows (9.29%). Estimated time left: 0h 0m 13s\n",
      "Processed 27000 rows (9.64%). Estimated time left: 0h 0m 13s\n",
      "Processed 28000 rows (10.00%). Estimated time left: 0h 0m 13s\n",
      "Processed 29000 rows (10.36%). Estimated time left: 0h 0m 13s\n",
      "Processed 30000 rows (10.71%). Estimated time left: 0h 0m 13s\n",
      "Processed 31000 rows (11.07%). Estimated time left: 0h 0m 12s\n",
      "Processed 32000 rows (11.43%). Estimated time left: 0h 0m 12s\n",
      "Processed 33000 rows (11.79%). Estimated time left: 0h 0m 12s\n",
      "Processed 34000 rows (12.14%). Estimated time left: 0h 0m 12s\n",
      "Processed 35000 rows (12.50%). Estimated time left: 0h 0m 12s\n",
      "Processed 36000 rows (12.86%). Estimated time left: 0h 0m 12s\n",
      "Processed 37000 rows (13.21%). Estimated time left: 0h 0m 12s\n",
      "Processed 38000 rows (13.57%). Estimated time left: 0h 0m 12s\n",
      "Processed 39000 rows (13.93%). Estimated time left: 0h 0m 12s\n",
      "Processed 40000 rows (14.29%). Estimated time left: 0h 0m 12s\n",
      "Processed 41000 rows (14.64%). Estimated time left: 0h 0m 12s\n",
      "Processed 42000 rows (15.00%). Estimated time left: 0h 0m 11s\n",
      "Processed 43000 rows (15.36%). Estimated time left: 0h 0m 11s\n",
      "Processed 44000 rows (15.71%). Estimated time left: 0h 0m 11s\n",
      "Processed 45000 rows (16.07%). Estimated time left: 0h 0m 11s\n",
      "Processed 46000 rows (16.43%). Estimated time left: 0h 0m 11s\n",
      "Processed 47000 rows (16.79%). Estimated time left: 0h 0m 11s\n",
      "Processed 48000 rows (17.14%). Estimated time left: 0h 0m 11s\n",
      "Processed 49000 rows (17.50%). Estimated time left: 0h 0m 11s\n",
      "Processed 50000 rows (17.86%). Estimated time left: 0h 0m 11s\n",
      "Processed 51000 rows (18.21%). Estimated time left: 0h 0m 11s\n",
      "Processed 52000 rows (18.57%). Estimated time left: 0h 0m 11s\n",
      "Processed 53000 rows (18.93%). Estimated time left: 0h 0m 11s\n",
      "Processed 54000 rows (19.29%). Estimated time left: 0h 0m 11s\n",
      "Processed 55000 rows (19.64%). Estimated time left: 0h 0m 11s\n",
      "Processed 56000 rows (20.00%). Estimated time left: 0h 0m 10s\n",
      "Processed 57000 rows (20.36%). Estimated time left: 0h 0m 10s\n",
      "Processed 58000 rows (20.71%). Estimated time left: 0h 0m 11s\n",
      "Processed 59000 rows (21.07%). Estimated time left: 0h 0m 10s\n",
      "Processed 60000 rows (21.43%). Estimated time left: 0h 0m 10s\n",
      "Processed 61000 rows (21.79%). Estimated time left: 0h 0m 10s\n",
      "Processed 62000 rows (22.14%). Estimated time left: 0h 0m 10s\n",
      "Processed 63000 rows (22.50%). Estimated time left: 0h 0m 10s\n",
      "Processed 64000 rows (22.86%). Estimated time left: 0h 0m 10s\n",
      "Processed 65000 rows (23.21%). Estimated time left: 0h 0m 10s\n",
      "Processed 66000 rows (23.57%). Estimated time left: 0h 0m 10s\n",
      "Processed 67000 rows (23.93%). Estimated time left: 0h 0m 10s\n",
      "Processed 68000 rows (24.29%). Estimated time left: 0h 0m 10s\n",
      "Processed 69000 rows (24.64%). Estimated time left: 0h 0m 10s\n",
      "Processed 70000 rows (25.00%). Estimated time left: 0h 0m 10s\n",
      "Processed 71000 rows (25.36%). Estimated time left: 0h 0m 9s\n",
      "Processed 72000 rows (25.71%). Estimated time left: 0h 0m 10s\n",
      "Processed 73000 rows (26.07%). Estimated time left: 0h 0m 10s\n",
      "Processed 74000 rows (26.43%). Estimated time left: 0h 0m 10s\n",
      "Processed 75000 rows (26.79%). Estimated time left: 0h 0m 9s\n",
      "Processed 76000 rows (27.14%). Estimated time left: 0h 0m 9s\n",
      "Processed 77000 rows (27.50%). Estimated time left: 0h 0m 9s\n",
      "Processed 78000 rows (27.86%). Estimated time left: 0h 0m 9s\n",
      "Processed 79000 rows (28.21%). Estimated time left: 0h 0m 9s\n",
      "Processed 80000 rows (28.57%). Estimated time left: 0h 0m 9s\n",
      "Processed 81000 rows (28.93%). Estimated time left: 0h 0m 9s\n",
      "Processed 82000 rows (29.29%). Estimated time left: 0h 0m 9s\n",
      "Processed 83000 rows (29.64%). Estimated time left: 0h 0m 9s\n",
      "Processed 84000 rows (30.00%). Estimated time left: 0h 0m 9s\n",
      "Processed 85000 rows (30.36%). Estimated time left: 0h 0m 9s\n",
      "Processed 86000 rows (30.71%). Estimated time left: 0h 0m 9s\n",
      "Processed 87000 rows (31.07%). Estimated time left: 0h 0m 9s\n",
      "Processed 88000 rows (31.43%). Estimated time left: 0h 0m 9s\n",
      "Processed 89000 rows (31.79%). Estimated time left: 0h 0m 9s\n",
      "Processed 90000 rows (32.14%). Estimated time left: 0h 0m 9s\n",
      "Processed 91000 rows (32.50%). Estimated time left: 0h 0m 9s\n",
      "Processed 92000 rows (32.86%). Estimated time left: 0h 0m 9s\n",
      "Processed 93000 rows (33.21%). Estimated time left: 0h 0m 9s\n",
      "Processed 94000 rows (33.57%). Estimated time left: 0h 0m 8s\n",
      "Processed 95000 rows (33.93%). Estimated time left: 0h 0m 8s\n",
      "Processed 96000 rows (34.28%). Estimated time left: 0h 0m 8s\n",
      "Processed 97000 rows (34.64%). Estimated time left: 0h 0m 8s\n",
      "Processed 98000 rows (35.00%). Estimated time left: 0h 0m 8s\n",
      "Processed 99000 rows (35.36%). Estimated time left: 0h 0m 8s\n",
      "Processed 100000 rows (35.71%). Estimated time left: 0h 0m 8s\n",
      "Processed 101000 rows (36.07%). Estimated time left: 0h 0m 8s\n",
      "Processed 102000 rows (36.43%). Estimated time left: 0h 0m 8s\n",
      "Processed 103000 rows (36.78%). Estimated time left: 0h 0m 8s\n",
      "Processed 104000 rows (37.14%). Estimated time left: 0h 0m 8s\n",
      "Processed 105000 rows (37.50%). Estimated time left: 0h 0m 8s\n",
      "Processed 106000 rows (37.86%). Estimated time left: 0h 0m 8s\n",
      "Processed 107000 rows (38.21%). Estimated time left: 0h 0m 8s\n",
      "Processed 108000 rows (38.57%). Estimated time left: 0h 0m 8s\n",
      "Processed 109000 rows (38.93%). Estimated time left: 0h 0m 8s\n",
      "Processed 110000 rows (39.28%). Estimated time left: 0h 0m 8s\n",
      "Processed 111000 rows (39.64%). Estimated time left: 0h 0m 8s\n",
      "Processed 112000 rows (40.00%). Estimated time left: 0h 0m 8s\n",
      "Processed 113000 rows (40.36%). Estimated time left: 0h 0m 8s\n",
      "Processed 114000 rows (40.71%). Estimated time left: 0h 0m 8s\n",
      "Processed 115000 rows (41.07%). Estimated time left: 0h 0m 8s\n",
      "Processed 116000 rows (41.43%). Estimated time left: 0h 0m 8s\n",
      "Processed 117000 rows (41.78%). Estimated time left: 0h 0m 8s\n",
      "Processed 118000 rows (42.14%). Estimated time left: 0h 0m 8s\n",
      "Processed 119000 rows (42.50%). Estimated time left: 0h 0m 7s\n",
      "Processed 120000 rows (42.86%). Estimated time left: 0h 0m 7s\n",
      "Processed 121000 rows (43.21%). Estimated time left: 0h 0m 7s\n",
      "Processed 122000 rows (43.57%). Estimated time left: 0h 0m 7s\n",
      "Processed 123000 rows (43.93%). Estimated time left: 0h 0m 7s\n",
      "Processed 124000 rows (44.28%). Estimated time left: 0h 0m 7s\n",
      "Processed 125000 rows (44.64%). Estimated time left: 0h 0m 7s\n",
      "Processed 126000 rows (45.00%). Estimated time left: 0h 0m 7s\n",
      "Processed 127000 rows (45.36%). Estimated time left: 0h 0m 7s\n",
      "Processed 128000 rows (45.71%). Estimated time left: 0h 0m 7s\n",
      "Processed 129000 rows (46.07%). Estimated time left: 0h 0m 7s\n",
      "Processed 130000 rows (46.43%). Estimated time left: 0h 0m 7s\n",
      "Processed 131000 rows (46.78%). Estimated time left: 0h 0m 7s\n",
      "Processed 132000 rows (47.14%). Estimated time left: 0h 0m 7s\n",
      "Processed 133000 rows (47.50%). Estimated time left: 0h 0m 7s\n",
      "Processed 134000 rows (47.86%). Estimated time left: 0h 0m 7s\n",
      "Processed 135000 rows (48.21%). Estimated time left: 0h 0m 7s\n",
      "Processed 136000 rows (48.57%). Estimated time left: 0h 0m 7s\n",
      "Processed 137000 rows (48.93%). Estimated time left: 0h 0m 7s\n",
      "Processed 138000 rows (49.28%). Estimated time left: 0h 0m 6s\n",
      "Processed 139000 rows (49.64%). Estimated time left: 0h 0m 6s\n",
      "Processed 140000 rows (50.00%). Estimated time left: 0h 0m 6s\n",
      "Processed 141000 rows (50.36%). Estimated time left: 0h 0m 6s\n",
      "Processed 142000 rows (50.71%). Estimated time left: 0h 0m 6s\n",
      "Processed 143000 rows (51.07%). Estimated time left: 0h 0m 6s\n",
      "Processed 144000 rows (51.43%). Estimated time left: 0h 0m 6s\n",
      "Processed 145000 rows (51.78%). Estimated time left: 0h 0m 6s\n",
      "Processed 146000 rows (52.14%). Estimated time left: 0h 0m 6s\n",
      "Processed 147000 rows (52.50%). Estimated time left: 0h 0m 6s\n",
      "Processed 148000 rows (52.86%). Estimated time left: 0h 0m 6s\n",
      "Processed 149000 rows (53.21%). Estimated time left: 0h 0m 6s\n",
      "Processed 150000 rows (53.57%). Estimated time left: 0h 0m 6s\n",
      "Processed 151000 rows (53.93%). Estimated time left: 0h 0m 6s\n",
      "Processed 152000 rows (54.28%). Estimated time left: 0h 0m 6s\n",
      "Processed 153000 rows (54.64%). Estimated time left: 0h 0m 6s\n",
      "Processed 154000 rows (55.00%). Estimated time left: 0h 0m 6s\n",
      "Processed 155000 rows (55.36%). Estimated time left: 0h 0m 6s\n",
      "Processed 156000 rows (55.71%). Estimated time left: 0h 0m 6s\n",
      "Processed 157000 rows (56.07%). Estimated time left: 0h 0m 6s\n",
      "Processed 158000 rows (56.43%). Estimated time left: 0h 0m 6s\n",
      "Processed 159000 rows (56.78%). Estimated time left: 0h 0m 6s\n",
      "Processed 160000 rows (57.14%). Estimated time left: 0h 0m 5s\n",
      "Processed 161000 rows (57.50%). Estimated time left: 0h 0m 5s\n",
      "Processed 162000 rows (57.86%). Estimated time left: 0h 0m 5s\n",
      "Processed 163000 rows (58.21%). Estimated time left: 0h 0m 5s\n",
      "Processed 164000 rows (58.57%). Estimated time left: 0h 0m 5s\n",
      "Processed 165000 rows (58.93%). Estimated time left: 0h 0m 5s\n",
      "Processed 166000 rows (59.28%). Estimated time left: 0h 0m 5s\n",
      "Processed 167000 rows (59.64%). Estimated time left: 0h 0m 5s\n",
      "Processed 168000 rows (60.00%). Estimated time left: 0h 0m 5s\n",
      "Processed 169000 rows (60.36%). Estimated time left: 0h 0m 5s\n",
      "Processed 170000 rows (60.71%). Estimated time left: 0h 0m 5s\n",
      "Processed 171000 rows (61.07%). Estimated time left: 0h 0m 5s\n",
      "Processed 172000 rows (61.43%). Estimated time left: 0h 0m 5s\n",
      "Processed 173000 rows (61.78%). Estimated time left: 0h 0m 5s\n",
      "Processed 174000 rows (62.14%). Estimated time left: 0h 0m 5s\n",
      "Processed 175000 rows (62.50%). Estimated time left: 0h 0m 5s\n",
      "Processed 176000 rows (62.86%). Estimated time left: 0h 0m 5s\n",
      "Processed 177000 rows (63.21%). Estimated time left: 0h 0m 5s\n",
      "Processed 178000 rows (63.57%). Estimated time left: 0h 0m 5s\n",
      "Processed 179000 rows (63.93%). Estimated time left: 0h 0m 4s\n",
      "Processed 180000 rows (64.28%). Estimated time left: 0h 0m 4s\n",
      "Processed 181000 rows (64.64%). Estimated time left: 0h 0m 4s\n",
      "Processed 182000 rows (65.00%). Estimated time left: 0h 0m 4s\n",
      "Processed 183000 rows (65.36%). Estimated time left: 0h 0m 4s\n",
      "Processed 184000 rows (65.71%). Estimated time left: 0h 0m 4s\n",
      "Processed 185000 rows (66.07%). Estimated time left: 0h 0m 4s\n",
      "Processed 186000 rows (66.43%). Estimated time left: 0h 0m 4s\n",
      "Processed 187000 rows (66.78%). Estimated time left: 0h 0m 4s\n",
      "Processed 188000 rows (67.14%). Estimated time left: 0h 0m 4s\n",
      "Processed 189000 rows (67.50%). Estimated time left: 0h 0m 4s\n",
      "Processed 190000 rows (67.86%). Estimated time left: 0h 0m 4s\n",
      "Processed 191000 rows (68.21%). Estimated time left: 0h 0m 4s\n",
      "Processed 192000 rows (68.57%). Estimated time left: 0h 0m 4s\n",
      "Processed 193000 rows (68.93%). Estimated time left: 0h 0m 4s\n",
      "Processed 194000 rows (69.28%). Estimated time left: 0h 0m 4s\n",
      "Processed 195000 rows (69.64%). Estimated time left: 0h 0m 4s\n",
      "Processed 196000 rows (70.00%). Estimated time left: 0h 0m 4s\n",
      "Processed 197000 rows (70.36%). Estimated time left: 0h 0m 4s\n",
      "Processed 198000 rows (70.71%). Estimated time left: 0h 0m 4s\n",
      "Processed 199000 rows (71.07%). Estimated time left: 0h 0m 4s\n",
      "Processed 200000 rows (71.43%). Estimated time left: 0h 0m 3s\n",
      "Processed 201000 rows (71.78%). Estimated time left: 0h 0m 3s\n",
      "Processed 202000 rows (72.14%). Estimated time left: 0h 0m 3s\n",
      "Processed 203000 rows (72.50%). Estimated time left: 0h 0m 3s\n",
      "Processed 204000 rows (72.86%). Estimated time left: 0h 0m 3s\n",
      "Processed 205000 rows (73.21%). Estimated time left: 0h 0m 3s\n",
      "Processed 206000 rows (73.57%). Estimated time left: 0h 0m 3s\n",
      "Processed 207000 rows (73.93%). Estimated time left: 0h 0m 3s\n",
      "Processed 208000 rows (74.28%). Estimated time left: 0h 0m 3s\n",
      "Processed 209000 rows (74.64%). Estimated time left: 0h 0m 3s\n",
      "Processed 210000 rows (75.00%). Estimated time left: 0h 0m 3s\n",
      "Processed 211000 rows (75.36%). Estimated time left: 0h 0m 3s\n",
      "Processed 212000 rows (75.71%). Estimated time left: 0h 0m 3s\n",
      "Processed 213000 rows (76.07%). Estimated time left: 0h 0m 3s\n",
      "Processed 214000 rows (76.43%). Estimated time left: 0h 0m 3s\n",
      "Processed 215000 rows (76.78%). Estimated time left: 0h 0m 3s\n",
      "Processed 216000 rows (77.14%). Estimated time left: 0h 0m 3s\n",
      "Processed 217000 rows (77.50%). Estimated time left: 0h 0m 3s\n",
      "Processed 218000 rows (77.85%). Estimated time left: 0h 0m 3s\n",
      "Processed 219000 rows (78.21%). Estimated time left: 0h 0m 3s\n",
      "Processed 220000 rows (78.57%). Estimated time left: 0h 0m 3s\n",
      "Processed 221000 rows (78.93%). Estimated time left: 0h 0m 2s\n",
      "Processed 222000 rows (79.28%). Estimated time left: 0h 0m 2s\n",
      "Processed 223000 rows (79.64%). Estimated time left: 0h 0m 2s\n",
      "Processed 224000 rows (80.00%). Estimated time left: 0h 0m 2s\n",
      "Processed 225000 rows (80.35%). Estimated time left: 0h 0m 2s\n",
      "Processed 226000 rows (80.71%). Estimated time left: 0h 0m 2s\n",
      "Processed 227000 rows (81.07%). Estimated time left: 0h 0m 2s\n",
      "Processed 228000 rows (81.43%). Estimated time left: 0h 0m 2s\n",
      "Processed 229000 rows (81.78%). Estimated time left: 0h 0m 2s\n",
      "Processed 230000 rows (82.14%). Estimated time left: 0h 0m 2s\n",
      "Processed 231000 rows (82.50%). Estimated time left: 0h 0m 2s\n",
      "Processed 232000 rows (82.85%). Estimated time left: 0h 0m 2s\n",
      "Processed 233000 rows (83.21%). Estimated time left: 0h 0m 2s\n",
      "Processed 234000 rows (83.57%). Estimated time left: 0h 0m 2s\n",
      "Processed 235000 rows (83.93%). Estimated time left: 0h 0m 2s\n",
      "Processed 236000 rows (84.28%). Estimated time left: 0h 0m 2s\n",
      "Processed 237000 rows (84.64%). Estimated time left: 0h 0m 2s\n",
      "Processed 238000 rows (85.00%). Estimated time left: 0h 0m 2s\n",
      "Processed 239000 rows (85.35%). Estimated time left: 0h 0m 2s\n",
      "Processed 240000 rows (85.71%). Estimated time left: 0h 0m 1s\n",
      "Processed 241000 rows (86.07%). Estimated time left: 0h 0m 1s\n",
      "Processed 242000 rows (86.43%). Estimated time left: 0h 0m 1s\n",
      "Processed 243000 rows (86.78%). Estimated time left: 0h 0m 1s\n",
      "Processed 244000 rows (87.14%). Estimated time left: 0h 0m 1s\n",
      "Processed 245000 rows (87.50%). Estimated time left: 0h 0m 1s\n",
      "Processed 246000 rows (87.85%). Estimated time left: 0h 0m 1s\n",
      "Processed 247000 rows (88.21%). Estimated time left: 0h 0m 1s\n",
      "Processed 248000 rows (88.57%). Estimated time left: 0h 0m 1s\n",
      "Processed 249000 rows (88.93%). Estimated time left: 0h 0m 1s\n",
      "Processed 250000 rows (89.28%). Estimated time left: 0h 0m 1s\n",
      "Processed 251000 rows (89.64%). Estimated time left: 0h 0m 1s\n",
      "Processed 252000 rows (90.00%). Estimated time left: 0h 0m 1s\n",
      "Processed 253000 rows (90.35%). Estimated time left: 0h 0m 1s\n",
      "Processed 254000 rows (90.71%). Estimated time left: 0h 0m 1s\n",
      "Processed 255000 rows (91.07%). Estimated time left: 0h 0m 1s\n",
      "Processed 256000 rows (91.43%). Estimated time left: 0h 0m 1s\n",
      "Processed 257000 rows (91.78%). Estimated time left: 0h 0m 1s\n",
      "Processed 258000 rows (92.14%). Estimated time left: 0h 0m 1s\n",
      "Processed 259000 rows (92.50%). Estimated time left: 0h 0m 1s\n",
      "Processed 260000 rows (92.85%). Estimated time left: 0h 0m 0s\n",
      "Processed 261000 rows (93.21%). Estimated time left: 0h 0m 0s\n",
      "Processed 262000 rows (93.57%). Estimated time left: 0h 0m 0s\n",
      "Processed 263000 rows (93.93%). Estimated time left: 0h 0m 0s\n",
      "Processed 264000 rows (94.28%). Estimated time left: 0h 0m 0s\n",
      "Processed 265000 rows (94.64%). Estimated time left: 0h 0m 0s\n",
      "Processed 266000 rows (95.00%). Estimated time left: 0h 0m 0s\n",
      "Processed 267000 rows (95.35%). Estimated time left: 0h 0m 0s\n",
      "Processed 268000 rows (95.71%). Estimated time left: 0h 0m 0s\n",
      "Processed 269000 rows (96.07%). Estimated time left: 0h 0m 0s\n",
      "Processed 270000 rows (96.43%). Estimated time left: 0h 0m 0s\n",
      "Processed 271000 rows (96.78%). Estimated time left: 0h 0m 0s\n",
      "Processed 272000 rows (97.14%). Estimated time left: 0h 0m 0s\n",
      "Processed 273000 rows (97.50%). Estimated time left: 0h 0m 0s\n",
      "Processed 274000 rows (97.85%). Estimated time left: 0h 0m 0s\n",
      "Processed 275000 rows (98.21%). Estimated time left: 0h 0m 0s\n",
      "Processed 276000 rows (98.57%). Estimated time left: 0h 0m 0s\n",
      "Processed 277000 rows (98.93%). Estimated time left: 0h 0m 0s\n",
      "Processed 278000 rows (99.28%). Estimated time left: 0h 0m 0s\n",
      "Processed 279000 rows (99.64%). Estimated time left: 0h 0m 0s\n",
      "Processed 280000 rows (100.00%). Estimated time left: 0h 0m 0s\n",
      "Matched 366 spectrum files and 549 scans.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Read the PeptideAtlas USIs from merged.txt\n",
    "usi_file = 'all_usi.txt'\n",
    "usi_data = []\n",
    "with open(usi_file, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(':')\n",
    "        if len(parts) == 6:\n",
    "            dataset, spectrum_file, scan, peptide_identification, peptide_charge = parts[1], parts[2].replace('_',''), parts[4], parts[5].split('/')[0], parts[5].split('/')[1]\n",
    "            usi_data.append([line.strip(), dataset, spectrum_file, scan, peptide_identification, peptide_charge])\n",
    "\n",
    "usi_df = pd.DataFrame(usi_data, columns=['USI', 'Dataset', 'Spectrum_File', 'Scan_Number', 'Peptide_Identification', 'Peptide_Charge'])\n",
    "usi_df.to_csv('df_print.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Create a dictionary for quick lookup\n",
    "usi_dict = {}\n",
    "for _, row in usi_df.iterrows():\n",
    "    key = (row['Spectrum_File'], row['Scan_Number'])\n",
    "    usi_dict[key] = row\n",
    "\n",
    "# Read the reanalysis results from MSGF-PLUS-AMBIGUITY-81a33a88-group_by_spectrum-main.tsv\n",
    "reanalysis_file = 'filtered.tsv'\n",
    "reanalysis_df = pd.read_csv(reanalysis_file, sep='\\t', low_memory=False)\n",
    "\n",
    "# Initialize columns for the output DataFrame\n",
    "reanalysis_df.insert(0, 'PeptideAtlas_USI', '')\n",
    "reanalysis_df.insert(1, 'PeptideAtlas_peptide', '')\n",
    "reanalysis_df.insert(2, 'PeptideAtlas_peptide_demod', '')\n",
    "reanalysis_df.insert(3, 'Peptide_match', '')\n",
    "reanalysis_df.insert(4, 'PeptideAtlas_charge', '')\n",
    "\n",
    "\n",
    "# Match spectra from the PeptideAtlas USIs lists to the reanalysis results\n",
    "all_MSGF_spectrum_files = set()\n",
    "    # Precompute set of all spectrum_file names in usi_dict for fast lookup\n",
    "if 'usi_spectrum_files_set' not in globals():\n",
    "    usi_spectrum_files_set = {k[0].replace('_','') for k in usi_dict.keys()}\n",
    "\n",
    "\n",
    "for index, row in reanalysis_df.iterrows():\n",
    "    original_filepath = row['opt_global_OriginalFilepath']\n",
    "    scan_number = str(row['opt_global_scan'])\n",
    "    \n",
    "    # Extract the spectrum file name from the original file path and remove the .mzML extension if present\n",
    "    spectrum_file = original_filepath.split('/')[-1].replace('.mzML', '').replace('_', '')\n",
    "    \n",
    "    key = (spectrum_file, scan_number)\n",
    "\n",
    "    if spectrum_file in usi_spectrum_files_set:\n",
    "        all_MSGF_spectrum_files.add(spectrum_file)\n",
    "        \n",
    "    if key in usi_dict:\n",
    "        usi_row = usi_dict[key]\n",
    "        reanalysis_df.at[index, 'PeptideAtlas_USI'] = usi_row['USI']\n",
    "        reanalysis_df.at[index, 'PeptideAtlas_peptide'] = usi_row['Peptide_Identification']\n",
    "        # Remove all substrings like \"[*]\" from PeptideAtlas_peptide\n",
    "        peptide_demod = re.sub(r'\\[.*?\\]', '', usi_row['Peptide_Identification']).replace('-', '')\n",
    "        reanalysis_df.at[index, 'PeptideAtlas_peptide_demod'] = peptide_demod\n",
    "        \n",
    "        # Set Peptide_match to 1 if PeptideAtlas_peptide_demod matches opt_global_UnmodPep, otherwise 0\n",
    "        reanalysis_df.at[index, 'Peptide_match'] = 1 if peptide_demod == row['opt_global_UnmodPep'] else 0\n",
    "        reanalysis_df.at[index, 'PeptideAtlas_charge'] = usi_row['Peptide_Charge']\n",
    "        #print(f'Matched file: {usi_row[\"Spectrum_File\"]}, Scan number: {usi_row[\"Scan_Number\"]}')\n",
    "\n",
    "    #elif spectrum_file is in usi_dict but scan_number is not in usi_dict, still add the spectrum file to all_MSGF_spectrum_files\n",
    "    if index % 1000 == 0:\n",
    "        if index == 0:\n",
    "            start_time = time.time()\n",
    "        else:\n",
    "            elapsed = time.time() - start_time\n",
    "            percent = (index + 1) / len(reanalysis_df) * 100\n",
    "            total_est = elapsed / (index + 1) * len(reanalysis_df)\n",
    "            remaining = total_est - elapsed\n",
    "            hrs, rem = divmod(remaining, 3600)\n",
    "            mins, secs = divmod(rem, 60)\n",
    "            print(f'Processed {index} rows ({percent:.2f}%). Estimated time left: {int(hrs)}h {int(mins)}m {int(secs)}s')\n",
    "\n",
    "    # Identify the datasets and spectrum files that have at least one match\n",
    "    # matched_datasets = set()\n",
    "matched_spectrum_files = set()\n",
    "matched_scans = set()\n",
    "\n",
    "for index, row in reanalysis_df.iterrows():\n",
    "\n",
    "    if row['PeptideAtlas_USI']:\n",
    "        usi_parts = row['PeptideAtlas_USI'].split(':')\n",
    "        if len(usi_parts) == 6:\n",
    "            dataset = usi_parts[1].replace('.mzML', '')\n",
    "            spectrum_file = usi_parts[2].replace('_', '')\n",
    "            matched_spectrum_files.add(spectrum_file)\n",
    "            matched_scans.add((spectrum_file, usi_parts[4]))\n",
    "print(f'Matched {len(matched_spectrum_files)} spectrum files and {len(matched_scans)} scans.')\n",
    "\n",
    "# Add empty USIs for unmatched spectra all at once\n",
    "new_rows = []\n",
    "for key, usi_row in usi_dict.items():\n",
    "    spectrum_file, scan_number = key\n",
    "    if (usi_row['Dataset'] == dataset_msgf or usi_row['Dataset'] == dataset_pa) and spectrum_file in all_MSGF_spectrum_files and key not in matched_scans:\n",
    "        new_rows.append({\n",
    "            'PeptideAtlas_USI': usi_row['USI'],\n",
    "            'PeptideAtlas_peptide': usi_row['Peptide_Identification'],\n",
    "            'PeptideAtlas_peptide_demod': re.sub(r'\\[.*?\\]', '', usi_row['Peptide_Identification']).replace('-', ''),\n",
    "            'Peptide_match': 0,\n",
    "            'PeptideAtlas_charge': usi_row['Peptide_Charge'],\n",
    "            'opt_global_OriginalFilepath': '',\n",
    "            'opt_global_scan': '',\n",
    "            'opt_global_UnmodPep': ''\n",
    "        })\n",
    "\n",
    "# Append all new rows to the DataFrame at once\n",
    "if new_rows:\n",
    "    reanalysis_df = pd.concat([reanalysis_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "\n",
    "output_file = 'example_reanalysis_spectrum.tsv'\n",
    "reanalysis_df.to_csv(output_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect all usi that are missed and put in the seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect USIs for new rows (MSGF misses)\n",
    "usi_msgf_misses = [row['PeptideAtlas_USI'] for row in new_rows]\n",
    "\n",
    "# Write the USIs to a separate file if there are any\n",
    "if usi_msgf_misses:\n",
    "    with open(dataset_msgf+'_usi_msgf_misses.txt', 'w') as f:\n",
    "        for usi in usi_msgf_misses:\n",
    "            # Remove underscore after 'Set' if present\n",
    "            parts = usi.split(':')\n",
    "            if len(parts) > 2 and 'Set_' in parts[2]:\n",
    "                parts[2] = parts[2].replace('Set_', 'Set')\n",
    "                usi_fixed = ':'.join(parts)\n",
    "            else:\n",
    "                usi_fixed = usi\n",
    "            f.write(f\"{usi_fixed}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if a set of usi is in matched set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of USIs to test\n",
    "# test_usis = [\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_2BRPhsFr3:scan:74886:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_8BRPhsFr2:scan:77613:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_7rernBRPhsFr3:scan:78829:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_5BRPhsFr2:scan:69880:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_2BRPhsFr3:scan:75698:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_2BRPhsFr2:scan:74664:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_9BRPhsFr3:scan:75218:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_10BRPhsFr29:scan:63274:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_6BRPhsFr29:scan:77646:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_11BRPhsFr3:scan:75311:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_2BRPhsFr3:scan:71253:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_5BRPhsFr29:scan:72852:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_8BRPhsFr3:scan:80420:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_6BRPhsFr3:scan:76155:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_6BRPhsFr3:scan:75958:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_3BRPhsFr2:scan:67089:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_6BRPhsFr32:scan:71409:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_1BRPhsFr2:scan:72866:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_9BRPhsFr30:scan:70005:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_2BRPhsFr2:scan:72270:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_6BRPhsFr3:scan:79999:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_6BRPhsFr3:scan:80172:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_10BRPhsFr28:scan:61486:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_9BRPhsFr1:scan:69260:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_6BRPhsFr2:scan:77737:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\",\n",
    "#     \"mzspec:PXD011967:ScltlMsclSet_6BRPhsFr29:scan:76324:EVMQQIK[TMT6plex]ILEATPLLESFGNAK[TMT6plex]/3\"\n",
    "# ]\n",
    "\n",
    "# # Normalize and extract (Spectrum_File, Scan_Number) keys from the test USIs\n",
    "# def extract_key_from_usi(usi):\n",
    "#     parts = usi.split(':')\n",
    "#     if len(parts) == 6:\n",
    "#         spectrum_file = parts[2].replace('_', '')\n",
    "#         scan_number = parts[4]\n",
    "#         return (spectrum_file, scan_number)\n",
    "#     return None\n",
    "\n",
    "# for usi in test_usis:\n",
    "#     key = extract_key_from_usi(usi)\n",
    "#     if key in matched_scans:\n",
    "#         print(f\"USI '{usi}' is a match in the PeptideAtlas USI set (usi_dict).\")\n",
    "#     else:\n",
    "#         # Check if the spectrum file and scan number exist in the MSGF results (all_MSGF_spectrum_files)\n",
    "#         spectrum_file, scan_number = key if key else (None, None)\n",
    "#         if spectrum_file in all_MSGF_spectrum_files:\n",
    "#             print(f\"USI '{usi}' is NOT in PeptideAtlas USI set, but spectrum file '{spectrum_file}' is present in MSGF results.\")\n",
    "#         else:\n",
    "#             print(f\"USI '{usi}' is NOT in PeptideAtlas USI set and spectrum file '{spectrum_file}' is NOT present in MSGF results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the first few entries in matched_scans\n",
    "# for i, scan in enumerate(matched_scans):\n",
    "#     print(scan)\n",
    "#     if i >= 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_in_all_MSGF_spectrum_files(filename, all_MSGF_spectrum_files):\n",
    "#     \"\"\"\n",
    "#     Check if the given filename matches any entry in all_MSGF_spectrum_files,\n",
    "#     using the same normalization as in the matching logic above.\n",
    "#     \"\"\"\n",
    "#     # Remove .mzML extension and underscores, as done in the matching logic\n",
    "#     normalized_filename = filename.split('/')[-1].replace('.mzML', '').replace('_', '')\n",
    "#     return normalized_filename in all_MSGF_spectrum_files\n",
    "\n",
    "#     # Example usage of is_in_all_MSGF_spectrum_files\n",
    "# test_filenames = [\n",
    "#     \"ScltlMsclSet_2BRPhsFr3\",\n",
    "#     \"ScltlMsclSet_8BRPhsFr2\",\n",
    "#     \"ScltlMsclSet_7rernBRPhsFr3\",\n",
    "#     \"ScltlMsclSet_5BRPhsFr2\",\n",
    "#     \"ScltlMsclSet_2BRPhsFr3\",\n",
    "#     \"ScltlMsclSet_2BRPhsFr2\",\n",
    "#     \"ScltlMsclSet_9BRPhsFr3\",\n",
    "#     \"ScltlMsclSet_10BRPhsFr29\",\n",
    "#     \"ScltlMsclSet_6BRPhsFr29\",\n",
    "#     \"ScltlMsclSet_11BRPhsFr3\",\n",
    "#     \"ScltlMsclSet_2BRPhsFr3\",\n",
    "#     \"ScltlMsclSet_5BRPhsFr29\",\n",
    "#     \"ScltlMsclSet_8BRPhsFr3\",\n",
    "#     \"ScltlMsclSet_6BRPhsFr3\",\n",
    "#     \"ScltlMsclSet_6BRPhsFr3\",\n",
    "#     \"ScltlMsclSet_3BRPhsFr2\",\n",
    "#     \"ScltlMsclSet_6BRPhsFr32\",\n",
    "#     \"ScltlMsclSet_1BRPhsFr2\",\n",
    "#     \"ScltlMsclSet_9BRPhsFr30\",\n",
    "#     \"ScltlMsclSet_2BRPhsFr2\",\n",
    "#     \"ScltlMsclSet_6BRPhsFr3\",\n",
    "#     \"ScltlMsclSet_6BRPhsFr3\",\n",
    "#     \"ScltlMsclSet_10BRPhsFr28\",\n",
    "#     \"ScltlMsclSet_9BRPhsFr1\",\n",
    "#     \"ScltlMsclSet_6BRPhsFr2\",\n",
    "#     \"ScltlMsclSet_6BRPhsFr29\"\n",
    "# ]\n",
    "\n",
    "# for fname in test_filenames:\n",
    "#     result = is_in_all_MSGF_spectrum_files(fname, all_MSGF_spectrum_files)\n",
    "#     print(f\"File '{fname}' in all_MSGF_spectrum_files? {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter rows where '#SpecFile' contains '01321_D12_P013127_S00_N12_R1'\n",
    "# filtered_rows = spectrum_df[spectrum_df['#SpecFile'].str.contains('01321_D12_P013127_S00_N12_R1', na=False)]\n",
    "\n",
    "# # Get the set of scan numbers from the 'opt_global_scan' column\n",
    "# scan_numbers = set(filtered_rows['opt_global_scan'].dropna().astype(int))\n",
    "\n",
    "# print(sorted(scan_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usi_file = 'all_usi.txt'\n",
    "# usi_data = []\n",
    "# with open(usi_file, 'r') as file:\n",
    "#     for line in file:\n",
    "#         parts = line.strip().split(':')\n",
    "#         if len(parts) == 6:\n",
    "#             dataset, spectrum_file, scan, peptide_identification, peptide_charge = parts[1], parts[2], parts[4], parts[5].split('/')[0], parts[5].split('/')[1]\n",
    "#             usi_data.append([line.strip(), dataset, spectrum_file, scan, peptide_identification, peptide_charge])\n",
    "#             # Print the first 5 examples from usi_data\n",
    "# # Print the first 5 examples from usi_data\n",
    "# for example in usi_data[:5]:\n",
    "#     print(example)\n",
    "\n",
    "# # Print the example that matches the specific USI\n",
    "# target_usi = \"mzspec:PXD010154:01321_D12_P013127_S00_N12_R1:scan:13514:FNSRTAELLSHHQVEIK/4\"\n",
    "# for example in usi_data:\n",
    "#     if example[0] == target_usi:\n",
    "#         print(\"Matched example:\", example)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peptide Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example_reanalysis_spectrum.tsv -> example_reanalysis_peptide.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiprocessing doeson't work here, please directly call compare_reanalyze_peptide.py in terminal or copy and run code below\n",
    "\n",
    "# import pandas as pd\n",
    "# from Bio import SeqIO\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "# import os\n",
    "\n",
    "# # Load the data\n",
    "# chunk_size = 10000\n",
    "# fasta_file = 'uniprotkb_human_proteome_UP000005640_with_isoforms_2024-10-08.fasta'\n",
    "\n",
    "# # Define a function to get the peptide sequence\n",
    "# def get_peptide_sequence(row):\n",
    "#     return row['PeptideAtlas_peptide_demod'] if pd.notna(row['PeptideAtlas_peptide_demod']) else row['opt_global_UnmodPep']\n",
    "\n",
    "# # Define a function to get the peptide charge\n",
    "# def get_peptide_charge(row):\n",
    "#     return row['PeptideAtlas_charge'] if pd.notna(row['PeptideAtlas_charge']) else row['charge']\n",
    "\n",
    "# # Define a function to get the protein ID from the sequence column \"tr|D9J307|D9J307_HUMAN\"\n",
    "# def get_protein_id_from_msgf(row):\n",
    "#     accession = row['accession']\n",
    "#     if isinstance(accession, str) and '|' in accession:\n",
    "#         parts = accession.split('|')\n",
    "#         if len(parts) > 1:\n",
    "#             return parts[1]\n",
    "#     return None\n",
    "\n",
    "# # Load protein sequences from fasta file\n",
    "# # Pre-compute and store sequences in a dictionary for faster lookups\n",
    "# protein_sequences = {}\n",
    "# for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "#     gene_id = next((part.split('=')[1] for part in record.description.split() if part.startswith('GN=')), 'UNKNOWN')\n",
    "#     protein_sequences[record.id] = {\n",
    "#         'gene_id': gene_id,\n",
    "#         'sequence': str(record.seq).replace('I', 'L')\n",
    "#     }\n",
    "\n",
    "# # Function to count matches in protein sequences\n",
    "# def count_matches(peptide, allow_mutation=False):\n",
    "#     peptide = peptide.replace('I', 'L')\n",
    "#     protein_ids = set()\n",
    "#     gene_ids = set()\n",
    "#     protein_list = []\n",
    "#     gene_list = []\n",
    "    \n",
    "#     for header, data in protein_sequences.items():\n",
    "#         gene_id = data['gene_id']\n",
    "#         sequence = data['sequence']\n",
    "#         protein_id = header.split('|')[1] if '|' in header else header\n",
    "        \n",
    "#         if allow_mutation:\n",
    "#             # Check for near-matches (SAAP)\n",
    "#             for i in range(len(sequence) - len(peptide) + 1):\n",
    "#                 window = sequence[i:i+len(peptide)]\n",
    "#                 if sum(1 for a, b in zip(peptide, window) if a != b) <= 1:\n",
    "#                     protein_ids.add(protein_id)\n",
    "#                     gene_ids.add(gene_id)\n",
    "#                     break\n",
    "#         else:\n",
    "#             # Check for exact matches\n",
    "#             if peptide in sequence:\n",
    "#                 protein_ids.add(protein_id)\n",
    "#                 gene_ids.add(gene_id)\n",
    "    \n",
    "#     # Return counts and lists\n",
    "#     return (\n",
    "#         len(protein_ids), \n",
    "#         len(gene_ids), \n",
    "#         ';'.join(protein_ids) if protein_ids else 'None', \n",
    "#         ';'.join(gene_ids) if gene_ids else 'UNKNOWN'\n",
    "#     )\n",
    "\n",
    "# # Function to process a peptide\n",
    "# def process_peptide(to_parallel_process):\n",
    "#     peptide, peptide_data, peptideatlas_df = to_parallel_process\n",
    "    \n",
    "#     # Get exact matches and SAAP matches\n",
    "#     num_proteins, num_genes, list_proteins, list_genes = count_matches(peptide)\n",
    "#     num_proteins_saap, num_genes_saap, list_proteins_saap, list_genes_saap = count_matches(peptide, allow_mutation=True)\n",
    "    \n",
    "#     # Build the output row\n",
    "#     peptide_row = {\n",
    "#         'Peptide sequence': peptide,\n",
    "#         'Peptide charge': peptide_data.apply(get_peptide_charge, axis=1).iloc[0],\n",
    "#         'Protein identifier': peptide_data.apply(get_protein_id_from_msgf, axis=1).iloc[0],\n",
    "#         'Num_specs_both': len(peptide_data[(pd.notna(peptide_data['PeptideAtlas_USI'])) & (pd.notna(peptide_data['sequence']))]),\n",
    "#         'Num_specs_MSGF': len(peptide_data[(pd.isna(peptide_data['PeptideAtlas_USI'])) & (pd.notna(peptide_data['sequence']))]),\n",
    "#         'Num_specs_PA': len(peptide_data[(pd.notna(peptide_data['PeptideAtlas_USI'])) & (pd.isna(peptide_data['sequence']))]),\n",
    "#         'PA_peptide': 1 if not peptideatlas_df[peptideatlas_df.iloc[:, 5] == peptide].empty else 0,\n",
    "#         'PA_psms': len(peptideatlas_df[peptideatlas_df.iloc[:, 5] == peptide]),\n",
    "#         'Num_proteins': num_proteins,\n",
    "#         'List_proteins': list_proteins,\n",
    "#         'Num_genes': num_genes,\n",
    "#         'List_genes': list_genes,\n",
    "#         'Num_proteins_saap': num_proteins_saap,\n",
    "#         'List_proteins_saap': list_proteins_saap,\n",
    "#         'Num_genes_saap': num_genes_saap,\n",
    "#         'List_genes_saap': list_genes_saap\n",
    "#     }\n",
    "#     #print(peptide_row)\n",
    "#     return peptide_row\n",
    "#     # Function to process an existing peptide\n",
    "# def process_existing_peptide(peptide, peptide_data, peptideatlas_df):\n",
    "#     # Build the output row without counting matches\n",
    "#     peptide_row = {\n",
    "#         'Peptide sequence': peptide,\n",
    "#         'Peptide charge': peptide_data.apply(get_peptide_charge, axis=1).iloc[0],\n",
    "#         'Protein identifier': peptide_data.apply(get_protein_id_from_msgf, axis=1).iloc[0],\n",
    "#         'Num_specs_both': len(peptide_data[(pd.notna(peptide_data['PeptideAtlas_USI'])) & (pd.notna(peptide_data['sequence']))]),\n",
    "#         'Num_specs_MSGF': len(peptide_data[(pd.isna(peptide_data['PeptideAtlas_USI'])) & (pd.notna(peptide_data['sequence']))]),\n",
    "#         'Num_specs_PA': len(peptide_data[(pd.notna(peptide_data['PeptideAtlas_USI'])) & (pd.isna(peptide_data['sequence']))]),\n",
    "#         'PA_peptide': 1 if not peptideatlas_df[peptideatlas_df.iloc[:, 5] == peptide].empty else 0,\n",
    "#         'PA_psms': len(peptideatlas_df[peptideatlas_df.iloc[:, 5] == peptide]),\n",
    "#         'Num_proteins': None,\n",
    "#         'List_proteins': None,\n",
    "#         'Num_genes': None,\n",
    "#         'List_genes': None,\n",
    "#         'Num_proteins_saap': None,\n",
    "#         'List_proteins_saap': None,\n",
    "#         'Num_genes_saap': None,\n",
    "#         'List_genes_saap': None\n",
    "#     }\n",
    "#     print(f\"Existing Peptide ID: {peptide_row['Peptide sequence']}\")\n",
    "#     return peptide_row\n",
    "# # Load the PeptideAtlas data\n",
    "# peptideatlas_df = pd.read_csv('PeptideAtlas_peptides.tsv', sep='\\t')\n",
    "\n",
    "# # Use a set to keep track of unique peptides\n",
    "# unique_peptides_set = set()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Open the output file\n",
    "#     peptide_file_exists = os.path.exists('example_reanalysis_peptide.tsv')\n",
    "#     with open('example_reanalysis_peptide.tsv', 'a+') as output_file:\n",
    "#         # Write the header\n",
    "#         if not peptide_file_exists:\n",
    "#             output_file.write('\\t'.join([\n",
    "#                 'Peptide sequence', 'Peptide charge', 'Protein identifier', 'Num_specs_both', 'Num_specs_MSGF', 'Num_specs_PA',\n",
    "#                 'PA_peptide', 'PA_psms', 'Num_proteins', 'List_proteins', 'Num_genes', 'List_genes', 'Num_proteins_saap',\n",
    "#                 'List_proteins_saap', 'Num_genes_saap', 'List_genes_saap'\n",
    "#             ]) + '\\n')\n",
    "        \n",
    "#         # Read the spectrum file in chunks\n",
    "#         # Check if the file exists\n",
    "#         if peptide_file_exists:\n",
    "#             # Read the existing peptides from the file\n",
    "#             with open('example_reanalysis_peptide.tsv', 'r') as existing_file:\n",
    "#                 # Skip the header\n",
    "#                 next(existing_file)\n",
    "#                 for line in existing_file:\n",
    "#                     peptide = line.split('\\t')[0]  # Extract the peptide sequence (first column)\n",
    "#                     unique_peptides_set.add(peptide)\n",
    "#         print(f\"Number of unique peptides added: {len(unique_peptides_set)}\")\n",
    "#         for chunk in pd.read_csv('example_reanalysis_spectrum.tsv', sep='\\t', chunksize=chunk_size):\n",
    "#             chunk['Peptide sequence'] = chunk.apply(get_peptide_sequence, axis=1)\n",
    "#             counter = 0\n",
    "#             total_peptides = len(chunk['Peptide sequence'].unique())\n",
    "\n",
    "#             def update_counter(result):\n",
    "#                 global counter\n",
    "#                 counter += 1\n",
    "#                 # print(f\"Processed {counter}/{total_peptides} peptides\")\n",
    "\n",
    "#             with Pool(cpu_count()) as pool:\n",
    "#                 peptides_to_process = [\n",
    "#                     (peptide, chunk[chunk['Peptide sequence'] == peptide], peptideatlas_df)\n",
    "#                     for peptide in chunk['Peptide sequence'].unique()\n",
    "#                 ]\n",
    "                \n",
    "#                 results = []\n",
    "#                 to_parallel_process = []\n",
    "#                 not_parallel_process = []\n",
    "\n",
    "#                 for peptide, peptide_data, pa_df in peptides_to_process:\n",
    "#                     if peptide not in unique_peptides_set:\n",
    "#                         #print(f\"Appending New Peptide: {peptide}\")\n",
    "#                         unique_peptides_set.add(peptide)\n",
    "#                         to_parallel_process.append((peptide, peptide_data, pa_df))\n",
    "#                     else:\n",
    "#                         #print(f\"Processing Existing Peptide: {peptide}\")\n",
    "#                         not_parallel_process.append((peptide, peptide_data, pa_df))\n",
    "\n",
    "#                 # Parallel process the new peptides\n",
    "#                 if to_parallel_process:\n",
    "#                     for result in pool.imap_unordered(process_peptide, to_parallel_process):\n",
    "#                         results.append(result)\n",
    "#                         update_counter(result)\n",
    "#                         print(f\"Processed {counter}/{total_peptides} peptides in chunk {chunk.index[0] // chunk_size + 1}\")\n",
    "\n",
    "#                 # Process the existing peptides\n",
    "#                 for peptide, peptide_data, pa_df in not_parallel_process:\n",
    "#                     results.append(process_existing_peptide(peptide, peptide_data, pa_df))\n",
    "#                     update_counter(None)\n",
    "#                     print(f\"Processed {counter}/{total_peptides} peptides in chunk {chunk.index[0] // chunk_size + 1}\")\n",
    "                    \n",
    "#                 for peptide_row in results:\n",
    "#                     peptide_sequence = peptide_row['Peptide sequence']\n",
    "                    \n",
    "#                     # Read the current output file content\n",
    "#                     output_file.seek(0)\n",
    "#                     lines = output_file.readlines()\n",
    "#                     print()\n",
    "#                     # Check if the peptide is already in the file\n",
    "#                     found = False\n",
    "#                     for i, line in enumerate(lines):\n",
    "#                         if line.startswith(peptide_sequence):\n",
    "#                             found = True\n",
    "#                             existing_data = line.strip().split('\\t')\n",
    "                            \n",
    "#                             # Update the existing line with new data\n",
    "#                             existing_data[3] = str(int(existing_data[3]) + peptide_row['Num_specs_both'])\n",
    "#                             existing_data[4] = str(int(existing_data[4]) + peptide_row['Num_specs_MSGF'])\n",
    "#                             existing_data[5] = str(int(existing_data[5]) + peptide_row['Num_specs_PA'])\n",
    "#                             existing_data[6] = str(int(existing_data[6]) + peptide_row['PA_peptide'])\n",
    "#                             existing_data[7] = str(int(existing_data[7]) + peptide_row['PA_psms'])\n",
    "                            \n",
    "#                             # Write the updated line back to the file\n",
    "#                             lines[i] = '\\t'.join(existing_data) + '\\n'\n",
    "#                             break\n",
    "                    \n",
    "#                     if not found:\n",
    "#                         # Append the new peptide row to the file\n",
    "#                         lines.append('\\t'.join(map(str, peptide_row.values())) + '\\n')\n",
    "                    \n",
    "#                     # Write the updated content back to the file\n",
    "#                     output_file.seek(0)\n",
    "#                     output_file.truncate()\n",
    "#                     output_file.writelines(lines)\n",
    "\n",
    "#                 pool.close()\n",
    "#                 pool.join()\n",
    "\n",
    "#             print(f'Processed chunk {chunk.index[0] // chunk_size + 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data from dictionaries.pkl\n",
    "with open('Human_proteome_dictionary_I_replaced_by_L.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Extract the prefix and suffix dictionaries\n",
    "prefix_dict = data[\"prefix_dict\"]\n",
    "\n",
    "# Print the dictionaries to verify\n",
    "print(\"Prefix Dictionary (first 5):\", dict(list(prefix_dict.items())[:5]))\n",
    "\n",
    "print(\"Keys in Prefix Dictionary:\", set(prefix_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionary look like: Prefix Dictionary (first 5): {'MDAA': [('A0A087WV00', 'DGKI', 1057),('A6NMZ7', 'COL6A6', 300),...... if all 4mer form peptide can be found in same protein and the length as showed in example here for prefix and length is >= the actual length of the peptide, the peptide is found in exact match, if there are <= 4  4-mers not found, then its in saap match. \n",
    "\n",
    "spectrum df look like this:PeptideAtlas_USI\tPeptideAtlas_peptide\tPeptideAtlas_peptide_demod\tPeptide_match\tPeptideAtlas_charge\trowid\tsequence\tPSM_ID\taccession\tunique\tdatabase\tdatabase_version\tsearch_engine\tmodifications\tretention_time\tcharge\texp_mass_to_charge\tcalc_mass_to_charge\tspectra_ref\tpre\tpost\tstart\tend\topt_global_valid\topt_global_invalid_reason\topt_global_AllGenes\topt_global_AllProteins\topt_global_CanonicalProteins\topt_global_DeNovoScore\topt_global_EValue\topt_global_FDR\topt_global_FragMethod\topt_global_IsotopeError\topt_global_MSGFScore\topt_global_NumPSMsForAllProteins\topt_global_NumPSMsForSpectrum\topt_global_NumPSMsForTopProtein\topt_global_NumPrecsForTopProtein\topt_global_NumSharedPSMsForTopProtein\topt_global_NumSharedPrecsForTopProtein\topt_global_NumUniquePSMsForTopProtein\topt_global_NumUniquePrecsForTopProtein\topt_global_OriginalFilepath\topt_global_PSM_sorting_score\topt_global_PeptideLength\topt_global_PrecQValue\topt_global_Precursor\topt_global_PrecursorError(ppm)\topt_global_Protein\topt_global_QValue\topt_global_SpecEValue\topt_global_SpecID\topt_global_Title\topt_global_TopCanonicalProtQValue\topt_global_TopCanonicalProtein\topt_global_TopGene\topt_global_TopGeneQValue\topt_global_TopGeneScore\topt_global_TopProtQValue\topt_global_TopProteinScore\topt_global_TopUniqueGene\topt_global_TopUniqueProtein\topt_global_UniqueGeneQValue\topt_global_UniqueProtQValue\topt_global_UnmodPep\topt_global_UnmodPepIL\topt_global_ambiguity_total_score\topt_global_collision_energy\topt_global_decoy\topt_global_filename\topt_global_first_second_unique_ratio\topt_global_first_unique_count\topt_global_first_unique_intensity\topt_global_kl_interpeak\topt_global_kl_strict\topt_global_kl_unstrict\topt_global_numberpsms\topt_global_ppm_error\topt_global_precursor_pep\topt_global_scan\topt_global_score\topt_global_second_unique_count\topt_global_second_unique_intensity\topt_global_sequence\topt_global_sorting_score\topt_global_spectrum_unique_key\topt_global_pass_threshold\topt_global_cv_MS:1002217_decoy_peptide\topt_global_cv_MS:1002354_PSM-level_q-value\t#SpecFile\tnativeID\tmod_set\tunique_PSM_ID\tmodified_sequence\tvariant\t_dyn_#AllGenes\t_dyn_#AllProteins\t_dyn_#CanonicalProteins\t_dyn_#DeNovoScore\t_dyn_#EValue\t_dyn_#FDR\t_dyn_#FragMethod\t_dyn_#IsotopeError\t_dyn_#MSGFScore\t_dyn_#NumPSMsForAllProteins\t_dyn_#NumPSMsForSpectrum\t_dyn_#NumPSMsForTopProtein\t_dyn_#NumPrecsForTopProtein\t_dyn_#NumSharedPSMsForTopProtein\t_dyn_#NumSharedPrecsForTopProtein\t_dyn_#NumUniquePSMsForTopProtein\t_dyn_#NumUniquePrecsForTopProtein\t_dyn_#OriginalFilepath\t_dyn_#PSM_sorting_score\t_dyn_#PeptideLength\t_dyn_#PrecQValue\t_dyn_#Precursor\t_dyn_#PrecursorError(ppm)\t_dyn_#Protein\t_dyn_#QValue\t_dyn_#SpecEValue\t_dyn_#SpecID\t_dyn_#Title\t_dyn_#TopCanonicalProtQValue\t_dyn_#TopCanonicalProtein\t_dyn_#TopGene\t_dyn_#TopGeneQValue\t_dyn_#TopGeneScore\t_dyn_#TopProtQValue\t_dyn_#TopProteinScore\t_dyn_#TopUniqueGene\t_dyn_#TopUniqueProtein\t_dyn_#UniqueGeneQValue\t_dyn_#UniqueProtQValue\t_dyn_#UnmodPep\t_dyn_#UnmodPepIL\t_dyn_#ambiguity_total_score\t_dyn_#collision_energy\t_dyn_#decoy\t_dyn_#filename\t_dyn_#first_second_unique_ratio\t_dyn_#first_unique_count\t_dyn_#first_unique_intensity\t_dyn_#kl_interpeak\t_dyn_#kl_strict\t_dyn_#kl_unstrict\t_dyn_#numberpsms\t_dyn_#ppm_error\t_dyn_#precursor_pep\t_dyn_#scan\t_dyn_#score\t_dyn_#second_unique_count\t_dyn_#second_unique_intensity\t_dyn_#sequence\t_dyn_#sorting_score\t_dyn_#spectrum_unique_key\t_dyn_#pass_threshold\t_dyn_#cv_MS:1002217_decoy_peptide\t_dyn_#cv_MS:1002354_PSM-level_q-value\tvalid\tinternalFilename\tnativeID_index\tnativeID_scan\tnativeID_index_1-based\tbaseFilename\tsequence_li\tmodified_sequence_li\tAllGenes\tAllProteins\tCanonicalProteins\tDeNovoScore\tEValue\tFDR\tFragMethod\tIsotopeError\tMSGFScore\tNumPSMsForAllProteins\tNumPSMsForSpectrum\tNumPSMsForTopProtein\tNumPrecsForTopProtein\tNumSharedPSMsForTopProtein\tNumSharedPrecsForTopProtein\tNumUniquePSMsForTopProtein\tNumUniquePrecsForTopProtein\tOriginalFilepath\tPSM_sorting_score\tPeptideLength\tPrecQValue\tPrecursor\tPrecursorError(ppm)\tProtein\tQValue\tSpecEValue\tSpecID\tTitle\tTopCanonicalProtQValue\tTopCanonicalProtein\tTopGene\tTopGeneQValue\tTopGeneScore\tTopProtQValue\tTopProteinScore\tTopUniqueGene\tTopUniqueProtein\tUniqueGeneQValue\tUniqueProtQValue\tUnmodPep\tUnmodPepIL\tambiguity_total_score\tcollision_energy\tdecoy\tfilename\tfirst_second_unique_ratio\tfirst_unique_count\tfirst_unique_intensity\tkl_interpeak\tkl_strict\tkl_unstrict\tnumberpsms\tppm_error\tprecursor_pep\tscan\tscore\tsecond_unique_count\tsecond_unique_intensity\tsequence.1\tsorting_score\tspectrum_unique_key\tpass_threshold\tcv_MS:1002217_decoy_peptide\tcv_MS:1002354_PSM-level_q-value\tid\n",
    "\t\t\t\t\t29.0\tVDDTQFVRFDSDAASQKMEPRAPW\t29.0\ttr|A0A1W2PSE7|A0A1W2PSE7_HUMAN\t\t\t\t\t\t\t4.0\t\t\tms_run[5]:scan=33297\t\t\t\t\tVALID\t\tHLA-A\ttr|A0A1W2PSE7|A0A1W2PSE7_HUMAN;tr|A0A0G2JI36|A0A0G2JI36_HUMAN;tr|Q5SUL5|Q5SUL5_HUMAN;tr|A0A0G2JL56|A0A0G2JL56_HUMAN;tr|A0A1W2PQD0|A0A1W2PQD0_HUMAN;tr|A0A0G2JIF2|A0A0G2JIF2_HUMAN\t\t229.0\t0.03459448\t0.009884998061765\tHCD\t0.0\t99.0\t4338.0\t1.0\t40.0\t11.0\t40.0\t11.0\t0.0\t0.0\tMSV000096130/ccms_peak/RAW/20170317_QEh1_LC2_FaMa_ChCh_SA_HLApII_RA957_3_R1.mzML\t1.46099319302198\t24.0\t0.0007213445863088\t699.8391\t10.727329\ttr|Q5SUL5|Q5SUL5_HUMAN\t0.009884998061765\t1.1862006e-09\tindex=27322\tScan Number: 33297\t\t\tHLA-A\t0.0\t4657.41082709346\t0.0\t71.1125831911783\tHLA-A\t\t0.0\t\tVDDTQFVRFDSDAASQKMEPRAPW\tVDDTQFVRFDSDAASQKMEPRAPW\t-1.0\t0.0\t0.0\tspec-00004.mgf\t-1.0\t-1.0\t-1.0\t-1.0\t-1.0\t-1.0\t1.0\t10.727329\tVDDTQFVRFDSDAASQKMEPRAPW.4\t33297\t1.46099319302198\t-1.0\t-1.0\tVDDTQFVRFDSDAASQKMEPRAPW.4\t1.46099319302198\tspec-00004.mgf:33297\t\t\t0.009884998061765\tMSV000096130/ccms_peak/RAW/20170317_QEh1_LC2_FaMa_ChCh_SA_HLApII_RA957_3_R1.mzML\tscan=33297\t\tscan=33297_VDDTQFVRFDSDAASQKMEPRAPW_null\tVDDTQFVRFDSDAASQKMEPRAPW\tVDDTQFVRFDSDAASQKMEPRAPW_4\tHLA-A\ttr|A0A1W2PSE7|A0A1W2PSE7_HUMAN;tr|A0A0G2JI36|A0A0G2JI36_HUMAN;tr|Q5SUL5|Q5SUL5_HUMAN;tr|A0A0G2JL56|A0A0G2JL56_HUMAN;tr|A0A1W2PQD0|A0A1W2PQD0_HUMAN;tr|A0A0G2JIF2|A0A0G2JIF2_HUMAN\t\t229.0\t0.03459448\t0.009884998061765\tHCD\t0.0\t99.0\t4338.0\t1.0\t40.0\t11.0\t40.0\t11.0\t0.0\t0.0\tMSV000096130/ccms_peak/RAW/20170317_QEh1_LC2_FaMa_ChCh_SA_HLApII_RA957_3_R1.mzML\t1.46099319302198\t24.0\t0.0007213445863088\t699.8391\t10.727329\ttr|Q5SUL5|Q5SUL5_HUMAN\t0.009884998061765\t1.1862006e-09\tindex=27322\tScan Number: 33297\t\t\tHLA-A\t0.0\t4657.41082709346\t0.0\t71.1125831911783\tHLA-A\t\t0.0\t\tVDDTQFVRFDSDAASQKMEPRAPW\tVDDTQFVRFDSDAASQKMEPRAPW\t-1.0\t0.0\t0.0\tspec-00004.mgf\t-1.0\t-1.0\t-1.0\t-1.0\t-1.0\t-1.0\t1.0\t10.727329\tVDDTQFVRFDSDAASQKMEPRAPW.4\t33297.0\t1.46099319302198\t-1.0\t-1.0\tVDDTQFVRFDSDAASQKMEPRAPW.4\t1.46099319302198\tspec-00004.mgf:33297\t\t\t0.009884998061765\tVALID\tf.MSV000096130/ccms_peak/RAW/20170317_QEh1_LC2_FaMa_ChCh_SA_HLApII_RA957_3_R1.mzML\t-1.0\t33297.0\t-1.0\t20170317_QEh1_LC2_FaMa_ChCh_SA_HLApII_RA957_3_R1\tVDDTQFVRFDSDAASQKMEPRAPW\tVDDTQFVRFDSDAASQKMEPRAPW\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t34.0\tEDLSSWTAADTAAQITQ\t34.0\ttr|S6AU73|S6AU73_HUMAN\t\t\t\t\t\t\t2.0\t\n",
    "\n",
    "sequence is a non repeating set read from PeptideAtlas_peptide_demod and sequence column, and charge is return row['PeptideAtlas_charge'] if pd.notna(row['PeptideAtlas_charge']) else row['charge'], "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mutate every aa to all 20 posibilities, and by the ned return the Union of each peptide return for protein and genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for one peptide\n",
    "def find_exact_matches(peptide, dictionary):\n",
    "    peptide = peptide.replace('I', 'L')\n",
    "    fourmers = [peptide[i:i+4] for i in range(len(peptide) - 3)]\n",
    "    matching_proteins = None\n",
    "    matching_genes = None\n",
    "\n",
    "    for fourmer in fourmers:\n",
    "        if fourmer in dictionary:\n",
    "            proteins = {entry[0] for entry in dictionary[fourmer]}\n",
    "            genes = {entry[1] for entry in dictionary[fourmer]}\n",
    "            if matching_proteins is None:\n",
    "                matching_proteins = proteins\n",
    "                matching_genes = genes\n",
    "            else:\n",
    "                matching_proteins &= proteins\n",
    "                filtered_genes = {gene for gene in genes if gene != 'Unknown'}\n",
    "                matching_genes &= filtered_genes\n",
    "        else:\n",
    "            return set(), set()  # If any 4-mer is not found, no exact match\n",
    "\n",
    "    return matching_proteins if matching_proteins else set(), matching_genes if matching_genes else set()\n",
    "\n",
    "# Function to find SAAP matches (one amino acid difference)\n",
    "def find_saap_matches(peptide, dictionary):\n",
    "    peptide = peptide.replace('I', 'L')\n",
    "    mutated_proteins = set()\n",
    "    mutated_genes = set()\n",
    "\n",
    "    for i in range(len(peptide)):\n",
    "        for aa in \"ACDEFGHKLMNPQRSTVWY\":  # Don't need to include 'I' here, because we already replaced 'I' with 'L'\n",
    "            mutated_peptide = peptide[:i] + aa + peptide[i+1:]\n",
    "            proteins, genes = find_exact_matches(mutated_peptide, dictionary)\n",
    "            mutated_proteins.update(proteins)\n",
    "            # Filter out 'Unknown' from genes\n",
    "            filtered_genes = {gene for gene in genes if gene != 'Unknown'}\n",
    "            mutated_genes.update(filtered_genes)\n",
    "\n",
    "    return mutated_proteins, mutated_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load the spectrum data to get unique peptides\n",
    "spectrum_file = 'example_reanalysis_spectrum.tsv'\n",
    "spectrum_df = pd.read_csv(spectrum_file, sep='\\t')\n",
    "\n",
    "# Extract unique peptides\n",
    "peptide_df = spectrum_df[['PeptideAtlas_peptide_demod', 'sequence', 'PeptideAtlas_charge', 'charge','accession']].copy()\n",
    "peptide_df['Peptide sequence'] = peptide_df.apply(\n",
    "    lambda row: row['PeptideAtlas_peptide_demod'] if pd.notna(row['PeptideAtlas_peptide_demod']) else row['sequence'], axis=1\n",
    ")\n",
    "peptide_df['Peptide charge'] = peptide_df.apply(\n",
    "    lambda row: row['PeptideAtlas_charge'] if pd.notna(row['PeptideAtlas_charge']) else row['charge'], axis=1\n",
    ")\n",
    "# Extract protein identifiers from the 'accession' column, handling both pipe-separated and non-pipe cases\n",
    "peptide_df['Protein identifier'] = peptide_df['accession'].apply(\n",
    "    lambda acc: ';'.join([\n",
    "        a.split('|')[1] if isinstance(a, str) and '|' in a else str(a)\n",
    "        for a in str(acc).split(';') if a and a != 'nan'\n",
    "    ]) if pd.notna(acc) else ''\n",
    ")\n",
    "# Group by 'Peptide sequence' and 'Peptide charge', stacking all protein identifiers (deduplicated) with ';'\n",
    "peptide_df = (\n",
    "    peptide_df\n",
    "    .groupby(['Peptide sequence', 'Peptide charge'], as_index=False)\n",
    "    .agg({'Protein identifier': lambda x: ';'.join(sorted(set(';'.join(x).split(';')) - {''}))})\n",
    ")\n",
    "\n",
    "output_file = \"example_reanalysis_peptide.tsv\"\n",
    "if os.path.exists(output_file):\n",
    "    existing = pd.read_csv(output_file, sep='\\t')\n",
    "    done_peptides = set(zip(existing['Peptide sequence'], existing['Peptide charge']))\n",
    "    peptide_df = peptide_df[~peptide_df.set_index(['Peptide sequence', 'Peptide charge']).index.isin(done_peptides)]\n",
    "    mode = 'a'\n",
    "    header = False\n",
    "else:\n",
    "    mode = 'w'\n",
    "    header = True\n",
    "\n",
    "if not peptide_df.empty:\n",
    "    total_peptides = len(peptide_df)\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    for index, row in peptide_df.iterrows():\n",
    "        peptide = row['Peptide sequence']\n",
    "\n",
    "        # Find exact matches\n",
    "        exact_matches_proteins, exact_matches_genes = find_exact_matches(peptide, prefix_dict)\n",
    "        num_proteins = len(exact_matches_proteins)\n",
    "        list_proteins = ';'.join(exact_matches_proteins) if exact_matches_proteins else 'None'\n",
    "        num_genes = len(exact_matches_genes)\n",
    "        list_genes = ';'.join(exact_matches_genes) if exact_matches_genes else 'None'\n",
    "\n",
    "        # Find SAAP matches\n",
    "        saap_matches_proteins, saap_matches_genes = find_saap_matches(peptide, prefix_dict)\n",
    "        num_proteins_saap = len(saap_matches_proteins)\n",
    "        list_proteins_saap = ';'.join(saap_matches_proteins) if saap_matches_proteins else 'None'\n",
    "        num_genes_saap = len(saap_matches_genes)\n",
    "        list_genes_saap = ';'.join(saap_matches_genes) if saap_matches_genes else 'None'\n",
    "\n",
    "        peptide_row = {\n",
    "            'Peptide sequence': peptide,\n",
    "            'Peptide charge': int(row['Peptide charge']),\n",
    "            'Protein identifier': row['Protein identifier'],\n",
    "            'Num_specs_both': 0,\n",
    "            'Num_specs_MSGF': 0,\n",
    "            'Num_specs_PA': 0,\n",
    "            'PA_peptide': 0,\n",
    "            'PA_psms': 0,\n",
    "            'Num_proteins': int(num_proteins),\n",
    "            'List_proteins': list_proteins,\n",
    "            'Num_genes': int(num_genes),\n",
    "            'List_genes': list_genes,\n",
    "            'Num_proteins_saap': int(num_proteins_saap),\n",
    "            'List_proteins_saap': list_proteins_saap,\n",
    "            'Num_genes_saap': int(num_genes_saap),\n",
    "            'List_genes_saap': list_genes_saap\n",
    "        }\n",
    "        results.append(peptide_row)\n",
    "\n",
    "        # Print progress\n",
    "        # Print progress based on number of new peptides processed\n",
    "        processed_peptides = len(results)\n",
    "        if processed_peptides % 10 == 0 or processed_peptides == total_peptides:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            percentage = (processed_peptides / total_peptides) * 100\n",
    "            estimated_time_left = (elapsed_time / processed_peptides) * (total_peptides - processed_peptides)\n",
    "            estimated_hours = int(estimated_time_left // 3600)\n",
    "            estimated_minutes = int((estimated_time_left % 3600) // 60)\n",
    "            estimated_seconds = int(estimated_time_left % 60)\n",
    "            print(f\"\\rProcessed {processed_peptides}/{total_peptides} peptides ({percentage:.2f}%). Elapsed time: {elapsed_time:.2f}s. Estimated time left: {estimated_hours} hours {estimated_minutes} minutes {estimated_seconds} seconds.\", end=\"\")\n",
    "\n",
    "    pd.DataFrame(results).to_csv(output_file, sep=\"\\t\", index=False, mode=mode, header=header)\n",
    "else:\n",
    "    print(\"No new peptides to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peptide_df.to_csv('example_peptide_reanalysis_updated1.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the peptide to search\n",
    "# peptide = \"QARERAEADVASLNR\"\n",
    "\n",
    "# # Replace 'I' with 'L' in the peptide for consistency\n",
    "# peptide = peptide.replace('I', 'L')\n",
    "# # Use find_exact_matches to find proteins and genes\n",
    "# exact_matches_proteins, exact_matches_genes = find_exact_matches(peptide, prefix_dict)\n",
    "# # Update the variables with the results\n",
    "# num_proteins = len(exact_matches_proteins)\n",
    "# num_genes = len(exact_matches_genes)\n",
    "\n",
    "# # Use find_saap_matches to find proteins and genes with one amino acid difference\n",
    "# saap_matches_proteins, saap_matches_genes = find_saap_matches(peptide, prefix_dict)\n",
    "\n",
    "# # Update the variables with the results\n",
    "# num_proteins_saap = len(saap_matches_proteins)\n",
    "# num_genes_saap = len(saap_matches_genes)\n",
    "\n",
    "# # Print out the proteins and genes for exact matches\n",
    "# print(\"Exact Match Proteins:\", exact_matches_proteins)\n",
    "# print(\"Exact Match Genes:\", exact_matches_genes)\n",
    "\n",
    "# # Print out the proteins and genes for SAAP matches\n",
    "# print(\"SAAP Match Proteins:\", saap_matches_proteins)\n",
    "# print(\"SAAP Match Genes:\", saap_matches_genes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fix peptide number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the peptide and spectrum data\n",
    "peptide_data_path = \"example_reanalysis_peptide.tsv\"\n",
    "spectrum_data_path = \"example_reanalysis_spectrum.tsv\"\n",
    "peptideatlas_df = pd.read_csv('PeptideAtlas_peptides.tsv', sep=\"\\t\")\n",
    "usi_file = 'all_usi.txt'\n",
    "\n",
    "with open(usi_file, 'r') as file:\n",
    "    peptides_pa = [re.sub(r'\\[.*?\\]', '', line.strip().split(':')[-1].split('/')[0].replace('-', '')) for line in file if len(line.strip().split(':')) == 6]\n",
    "\n",
    "peptide_df = pd.read_csv(peptide_data_path, sep=\"\\t\")\n",
    "spectrum_df = pd.read_csv(spectrum_data_path, sep=\"\\t\")\n",
    "\n",
    "\n",
    "def get_protein_id_from_pa(row):\n",
    "    peptide_sequence = row['Peptide sequence']\n",
    "    matching_rows = peptideatlas_df[peptideatlas_df.iloc[:, 5] == peptide_sequence]\n",
    "    if not matching_rows.empty:\n",
    "        return matching_rows.iloc[0, 0]  # Return the protein ID from column 0\n",
    "    return None\n",
    "output_df = pd.DataFrame()\n",
    "\n",
    "# Function to update the specified columns\n",
    "def update_peptide_data(peptide_df, spectrum_df):\n",
    "    # Iterate through each peptide in the peptide dataframe\n",
    "    pa_observations_df = pd.read_csv('PA_observations.csv')\n",
    "    for index, row in peptide_df.iterrows():\n",
    "        # Print progress update\n",
    "        if index % 100 == 0:\n",
    "            print(f\"Processing peptide {index+1}/{len(peptide_df)} ({(index+1)/len(peptide_df)*100:.1f}%)\")\n",
    "        peptide_sequence = row['Peptide sequence']\n",
    "        peptide_charge = row['Peptide charge']\n",
    "        \n",
    "        # Filter spectrum data for the current peptide\n",
    "        spectrum_subset = spectrum_df[\n",
    "            ((spectrum_df['sequence'] == peptide_sequence) & \n",
    "             ((spectrum_df['charge'] == peptide_charge) | pd.isna(spectrum_df['charge']))) |\n",
    "            ((spectrum_df['PeptideAtlas_peptide_demod'] == peptide_sequence) & \n",
    "             ((spectrum_df['PeptideAtlas_charge'] == peptide_charge) | pd.isna(spectrum_df['PeptideAtlas_charge'])))\n",
    "        ]\n",
    "        \n",
    "        # Update the columns\n",
    "        num_specs_both = len(spectrum_subset[(pd.notna(spectrum_subset['PeptideAtlas_USI'])) & (pd.notna(spectrum_subset['sequence']))])\n",
    "        num_specs_msgf = len(spectrum_subset[(pd.isna(spectrum_subset['PeptideAtlas_USI'])) & (pd.notna(spectrum_subset['sequence']))])\n",
    "        num_specs_pa = len(spectrum_subset[(pd.notna(spectrum_subset['PeptideAtlas_USI'])) & (pd.isna(spectrum_subset['sequence']))])\n",
    "        pa_peptide = 0\n",
    "\n",
    "        if peptide_sequence in pa_observations_df['Sequence'].values:\n",
    "            pa_peptide = 1\n",
    "        # Count PSMs from all_usi.txt\n",
    "        pa_psms = peptides_pa.count(peptide_sequence)\n",
    "        # try:\n",
    "        #     with open('all_usii.txt', 'r') as usi_file:\n",
    "        #         for line in usi_file:\n",
    "        #             usi = line.strip()\n",
    "        #             # Extract and demodify the peptide sequence from the USI\n",
    "        #             if \":\" not in usi or \"/\" not in usi:\n",
    "        #                 continue\n",
    "                    \n",
    "        #             # Extract sequence part between last colon and slash\n",
    "        #             parts = usi.split(':')\n",
    "        #             if len(parts) < 2:\n",
    "        #                 continue\n",
    "                    \n",
    "        #             seq_part = parts[-1].split('/')[0]\n",
    "                    \n",
    "        #             # Remove modifications (text in square brackets)\n",
    "        #             demod_seq = re.sub(r'\\[.*?\\]', '', seq_part)\n",
    "                    \n",
    "        #             # If peptide matches the current peptide sequence\n",
    "        #             if demod_seq == peptide_sequence:\n",
    "        #                 pa_psms += 1\n",
    "        # except FileNotFoundError:\n",
    "        # pa_psms = len(spectrum_subset[spectrum_subset['sequence'] == peptide_sequence])\n",
    "        \n",
    "        # Update the peptide dataframe\n",
    "        \n",
    "        protein_identifier = row['Protein identifier']\n",
    "        if isinstance(protein_identifier, str):\n",
    "            peptide_df.at[index, 'Protein identifier MSGF'] = protein_identifier.split('-')[0]\n",
    "        else:\n",
    "            peptide_df.at[index, 'Protein identifier MSGF'] = None\n",
    "        peptide_df.at[index, 'Protein identifier PA'] = \";\".join(peptideatlas_df[peptideatlas_df.iloc[:, 5] == peptide_sequence].iloc[:, 0].dropna().unique())\n",
    "        peptide_df.at[index, 'Num_specs_both'] = num_specs_both\n",
    "        peptide_df.at[index, 'Num_specs_MSGF'] = num_specs_msgf\n",
    "        peptide_df.at[index, 'Num_specs_PA'] = num_specs_pa\n",
    "        peptide_df.at[index, 'PA_peptide'] = pa_peptide\n",
    "        peptide_df.at[index, 'PA_psms'] = pa_psms\n",
    "\n",
    "    return peptide_df\n",
    "\n",
    "# Update the peptide data\n",
    "updated_peptide_df = update_peptide_data(peptide_df, spectrum_df)\n",
    "\n",
    "# Save the updated dataframe\n",
    "updated_peptide_df.to_csv(\"example_peptide_reanalysis_updated.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fix protein identifier msgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the protein from 'accession' in spectrum_df and add to peptide_df as 'Protein identifier'\n",
    "import pandas as pd\n",
    "\n",
    "# Load the peptide and spectrum data\n",
    "peptide_file = \"example_peptide_reanalysis_updated.tsv\"\n",
    "spectrum_file = \"example_reanalysis_spectrum.tsv\"\n",
    "\n",
    "peptide_df = pd.read_csv(peptide_file, sep=\"\\t\")\n",
    "spectrum_df = pd.read_csv(spectrum_file, sep=\"\\t\")\n",
    "\n",
    "# For each peptide, find all unique protein identifiers from spectrum_df and update the peptide_df\n",
    "protein_ids_all = []\n",
    "total = len(peptide_df)\n",
    "for idx, row in peptide_df.iterrows():\n",
    "    peptide = row['Peptide sequence']\n",
    "    # Find all matching rows in spectrum_df\n",
    "    matches = spectrum_df[\n",
    "        (spectrum_df['PeptideAtlas_peptide_demod'] == peptide) | \n",
    "        (spectrum_df['sequence'] == peptide)\n",
    "    ]\n",
    "    # Extract unique protein identifiers from 'accession' column\n",
    "    protein_ids = matches['accession'].dropna().unique()\n",
    "    # Parse to get only the protein accession (middle part if pipe-separated)\n",
    "    protein_ids = [\n",
    "        acc.split('|')[1] if isinstance(acc, str) and '|' in acc else acc\n",
    "        for acc in protein_ids\n",
    "    ]\n",
    "    # Join all unique protein ids with ';'\n",
    "    protein_ids_str = \";\".join(sorted(set(protein_ids))) if protein_ids else \"\"\n",
    "    protein_ids_all.append(protein_ids_str)\n",
    "    # Print progress\n",
    "    if (idx + 1) % 10 == 0 or (idx + 1) == total:\n",
    "        percent = 100 * (idx + 1) / total\n",
    "        print(f\"\\rProcessing: {percent:.2f}% ({idx + 1}/{total})\", end='', flush=True)\n",
    "print()  # for newline after progress\n",
    "\n",
    "# Add/replace the column in peptide_df\n",
    "peptide_df['Protein identifier'] = protein_ids_all\n",
    "\n",
    "# Save the updated dataframe\n",
    "peptide_df.to_csv(peptide_file, sep=\"\\t\", index=False)\n",
    "\n",
    "# Load the peptide file\n",
    "peptide_df = pd.read_csv(peptide_file, sep=\"\\t\")\n",
    "\n",
    "# Copy 'Protein identifier' column to 'Protein identifier MSGF'\n",
    "peptide_df['Protein identifier MSGF'] = peptide_df['Protein identifier']\n",
    "\n",
    "# Save the updated dataframe\n",
    "peptide_df.to_csv(peptide_file, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "peptide_file = 'example_peptide_reanalysis_updated.tsv'\n",
    "protein_file = 'PeptideAtlas_proteins_not_in_MassIVE.tsv'\n",
    "\n",
    "peptide_df = pd.read_csv(peptide_file, sep='\\t')\n",
    "protein_df = pd.read_csv(protein_file, sep='\\t')\n",
    "\n",
    "# Initialize the result dataframe\n",
    "result_columns = [\n",
    "    'Protein', 'Num_peptides_both', 'Num_peptides_MSGF', 'Num_peptides_PA',\n",
    "    'Num_unique_both', 'Num_unique_MSGF', 'Num_unique_PA',\n",
    "    'Num_specs_both', 'Num_specs_MSGF', 'Num_specs_PA'\n",
    "]\n",
    "result_df = pd.DataFrame(columns=result_columns)\n",
    "\n",
    "# Iterate over each protein\n",
    "result_list = []\n",
    "for protein in protein_df['nextprot_accession']:\n",
    "    # Filter peptides that match the current protein\n",
    "\n",
    "    matched_peptides_both = peptide_df[\n",
    "        peptide_df['Protein identifier MSGF'].str.contains(protein, na=False) &\n",
    "        peptide_df['Protein identifier PA'].str.contains(protein, na=False)\n",
    "    ]\n",
    "    matched_peptides_msgf = peptide_df[\n",
    "        peptide_df['Protein identifier MSGF'].str.contains(protein, na=False)\n",
    "    ]\n",
    "    matched_peptides_PA = peptide_df[\n",
    "        peptide_df['Protein identifier PA'].str.contains(protein, na=False)\n",
    "    ]\n",
    "    matched_peptides = pd.concat([matched_peptides_msgf, matched_peptides_PA]).drop_duplicates()\n",
    "    \n",
    "    # Calculate the required counts and sums\n",
    "    # Case 1: Peptides found in both MSGF and PA for this protein\n",
    "    num_peptides_both = len(matched_peptides_both[matched_peptides_both['Num_specs_both'] > 0])\n",
    "    # Case 2: Peptides found in MSGF for this protein (regardless of PA)\n",
    "    num_peptides_MSGF = len(matched_peptides_msgf[matched_peptides_msgf['Num_specs_MSGF'] > 0])\n",
    "    # Case 3: Peptides found in PA for this protein (regardless of MSGF)\n",
    "    num_peptides_PA = len(matched_peptides_PA[matched_peptides_PA['Num_specs_PA'] > 0])\n",
    "\n",
    "    # Unique peptides (Num_genes_saap == 1) for each case\n",
    "    num_unique_both = len(matched_peptides_both[(matched_peptides_both['Num_specs_both'] > 0) & (matched_peptides_both['Num_genes_saap'] == 1)])\n",
    "    num_unique_MSGF = len(matched_peptides_msgf[(matched_peptides_msgf['Num_specs_MSGF'] > 0) & (matched_peptides_msgf['Num_genes_saap'] == 1)])\n",
    "    num_unique_PA = len(matched_peptides_PA[(matched_peptides_PA['Num_specs_PA'] > 0) & (matched_peptides_PA['Num_genes_saap'] == 1)])\n",
    "\n",
    "    # Sum of spectra for each case (sum over the respective peptide sets)\n",
    "    num_specs_both = matched_peptides_both['Num_specs_both'].sum()\n",
    "    num_specs_MSGF = matched_peptides_msgf['Num_specs_MSGF'].sum()\n",
    "    num_specs_PA = matched_peptides_PA['Num_specs_PA'].sum()\n",
    "    # Append the results to the result list\n",
    "    result_list.append({\n",
    "        'Protein': protein,\n",
    "        'Num_peptides_both': num_peptides_both,\n",
    "        'Num_peptides_MSGF': num_peptides_MSGF,\n",
    "        'Num_peptides_PA': num_peptides_PA,\n",
    "        'Num_unique_both': num_unique_both,\n",
    "        'Num_unique_MSGF': num_unique_MSGF,\n",
    "        'Num_unique_PA': num_unique_PA,\n",
    "        'Num_specs_both': num_specs_both,\n",
    "        'Num_specs_MSGF': num_specs_MSGF,\n",
    "        'Num_specs_PA': num_specs_PA\n",
    "    })\n",
    "\n",
    "# Convert the result list to a dataframe\n",
    "result_df = pd.concat([pd.DataFrame([result]) for result in result_list], ignore_index=True)\n",
    "\n",
    "# Save the result to a new file in TSV format\n",
    "result_df.to_csv('example_protein_reanalysis.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Check ambiguity filterintg:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the two lists\n",
    "# list1 = [\n",
    "#     \"A0A0C4DH55\", \"Q6UX40\", \"O95406\", \"Q9Y284\", \"A0A075B6K0\", \"Q8N112\", \"A0A0C4DH41\", \"Q96FC9\", \"Q96DS6\", \"Q96MN9\",\n",
    "#     \"Q6UWI2\", \"Q5UAW9\", \"Q8WV19\", \"Q7Z4L0\", \"Q5T1S8\", \"Q08AH3\", \"A0A1B0GUW7\", \"A0A2R8Y4L2\", \"P01772\", \"Q02161\",\n",
    "#     \"P0CG38\", \"P18089\", \"P17538\", \"B6A8C7\", \"O95484\", \"O75360\", \"Q8N131\", \"Q86T20\", \"A0A2R8YCJ5\", \"P49682\"\n",
    "# ]\n",
    "\n",
    "# list2 = [\n",
    "#     \"Q96MN9\", \"P01742\", \"A0A087WSY4\", \"A0A0C4DH68\", \"Q6UX40\", \"P0CG39\", \"Q6UWI2\", \"Q5UAW9\", \"Q8WV19\", \"P68104\",\n",
    "#     \"A0A1B0GUW7\", \"A0A2R8Y4L2\", \"P01599\", \"Q8N112\", \"A0A0C4DH41\", \"A0A0C4DH43\", \"A0A0C4DH72\", \"P01709\", \"P01624\",\n",
    "#     \"P01772\", \"A0A075B6H8\", \"P0CG38\", \"P18089\", \"O95484\", \"O75360\", \"Q96DS6\", \"P62834\", \"Q8N131\", \"A0A0B4J2H0\",\n",
    "#     \"A0A2R8YCJ5\", \"A0A0C4DH69\", \"P49682\", \"Q9Y4E1\"\n",
    "# ]\n",
    "\n",
    "# # Convert to sets\n",
    "# set1 = set(list1)\n",
    "# set2 = set(list2)\n",
    "\n",
    "# # Items in list1 but not in list2\n",
    "# diff1 = sorted(set1 - set2)\n",
    "# # Items in list2 but not in list1\n",
    "# diff2 = sorted(set2 - set1)\n",
    "\n",
    "# print(\"Items in list1 but not in list2:\")\n",
    "# print(diff1)\n",
    "# print(\"\\nItems in list2 but not in list1:\")\n",
    "# print(diff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import time\n",
    "\n",
    "# filename = 'ambiguity_merged.tsv'\n",
    "# target = '01781_F03_P018699_S00_N22_R1'\n",
    "# found = False\n",
    "\n",
    "# # Get total number of lines (excluding header)\n",
    "# with open(filename, 'r') as f:\n",
    "#     total_lines = sum(1 for _ in f) - 1\n",
    "\n",
    "# chunksize = 10000\n",
    "# lines_checked = 0\n",
    "# for chunk in pd.read_csv(filename, sep='\\t', chunksize=chunksize, low_memory=False):\n",
    "#     mask = chunk.apply(lambda row: row.astype(str).str.contains(target, na=False).any(), axis=1)\n",
    "#     matching_rows = chunk[mask]\n",
    "#     lines_checked += len(chunk)\n",
    "#     percent = (lines_checked / total_lines) * 100\n",
    "#     if lines_checked == chunksize:\n",
    "#         start_time = time.time()\n",
    "#     elif lines_checked > chunksize:\n",
    "#         elapsed = time.time() - start_time\n",
    "#         est_total = elapsed / (lines_checked / total_lines)\n",
    "#         est_left = est_total - elapsed\n",
    "#         hrs, rem = divmod(est_left, 3600)\n",
    "#         mins, secs = divmod(rem, 60)\n",
    "#         print(f\"\\rChecked {lines_checked} / {total_lines} lines... ({percent:.2f}%) | Estimated time left: {int(hrs)}h {int(mins)}m {int(secs)}s\", end='')\n",
    "#     else:\n",
    "#         print(f\"\\rChecked {lines_checked} / {total_lines} lines... ({percent:.2f}%)\", end='')\n",
    "\n",
    "#     if not matching_rows.empty:\n",
    "#         print(matching_rows)\n",
    "#         found = True\n",
    "#         break\n",
    "\n",
    "# if not found:\n",
    "#     print(\"\\nnot found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output only proteins should be found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of proteins to filter (as provided)\n",
    "#TODO: Replace with the actual list of proteins you want to filter\n",
    "protein_list = [\n",
    "    \"Q14409\", \"A0A0K0K1C4\", \"A0M8Q6\", \"P0DJD3\", \"P06310\", \"A0A075B6R2\", \"Q96MN9\", \"P01700\", \"P01742\", \"P0DI81\",\n",
    "    \"P22392\", \"A0A087WSY4\", \"P61326\", \"Q9BRT8\", \"A0A0C4DH68\", \"A0A075B6K2\", \"A0A0C4DH55\", \"A0A5B6\", \"Q15413\",\n",
    "    \"Q6UX40\", \"P01615\", \"Q5EG05\", \"A0A075B6W5\", \"A0A075B6I7\", \"P69892\", \"Q6IS14\", \"P68133\", \"Q5VTL7\", \"P0CG39\",\n",
    "    \"P0C0S5\", \"P0DMV8\", \"Q16617\", \"O95406\", \"Q6UWI2\", \"P0DP23\", \"O95626\", \"Q5UAW9\", \"P0CG47\", \"Q96L21\", \"Q9BUW7\",\n",
    "    \"Q9H1U9\", \"Q8WV19\", \"Q56UQ5\", \"Q7Z4L0\", \"A0A075B767\", \"P35212\", \"Q5T1S8\", \"P47813\", \"P68104\", \"Q08AH3\",\n",
    "    \"B0FP48\", \"Q9Y284\", \"A0A1B0GUW7\", \"P06331\", \"A0A075B6K0\", \"A0A2R8Y4L2\", \"P01599\", \"Q8N112\", \"A0A0C4DH41\",\n",
    "    \"Q96IZ6\", \"A0A0C4DH43\", \"Q6P1Q9\", \"P0DPH7\", \"Q49MI3\", \"Q86VW1\", \"A0A0C4DH72\", \"A0A0C4DH35\", \"A0A0A0MRZ8\",\n",
    "    \"P01614\", \"P04430\", \"P01824\", \"P01709\", \"P01624\", \"B2RPK0\", \"Q96FC9\", \"A0A2R8Y619\", \"Q6P461\", \"P01772\",\n",
    "    \"Q6ZUT3\", \"Q02161\", \"A0A087WSY6\", \"A0A075B6S6\", \"P62837\", \"Q9NUN7\", \"P01768\", \"Q8NFI4\", \"A0A075B6N1\",\n",
    "    \"A0A075B6H7\", \"P01825\", \"A0A075B6H8\", \"Q6S8J3\", \"A0A075B6R9\", \"P0CG38\", \"P18089\", \"P0DME0\", \"Q13885\",\n",
    "    \"P17538\", \"O14604\", \"P59666\", \"B6A8C7\", \"P09341\", \"P98187\", \"P13164\", \"P50502\", \"O95484\", \"P68431\",\n",
    "    \"P68371\", \"O75360\", \"P20231\", \"Q96DS6\", \"A8K0Z3\", \"Q6UXB3\", \"Q9H0U4\", \"P0CW19\", \"Q9BQ87\", \"P0DTL6\",\n",
    "    \"Q8IZF0\", \"P62834\", \"Q7Z602\", \"A1L157\", \"Q8IWL1\", \"Q8IZP2\", \"A0A0C4DH42\", \"P01767\", \"P01593\", \"Q71UI9\",\n",
    "    \"Q8N131\", \"A6NK59\", \"Q9BY64\", \"A0A1B0GUS4\", \"A0A0C4DH25\", \"A0A0C4DH73\", \"P61224\", \"A0A0C4DH34\", \"P01601\",\n",
    "    \"A0A0B4J2H0\", \"Q9UQ74\", \"P61254\", \"Q147U7\", \"P04433\", \"Q9Y6W8\", \"A0A0C4DH36\", \"P32189\", \"P01597\",\n",
    "    \"A0A0A6YYC5\", \"Q96IZ2\", \"A0A0B4J1X8\", \"Q86T20\", \"O43374\", \"A0A2R8YCJ5\", \"Q8TCE9\", \"Q9BVA1\", \"A0A0C4DH69\",\n",
    "    \"P0CG34\", \"P86790\", \"P49682\", \"P0DTL5\", \"Q4G0N0\", \"P0DMM9\", \"A0A0A0MT36\", \"Q9Y4E1\", \"P0DSN7\", \"P01611\",\n",
    "    \"Q5VYV0\", \"A5A3E0\", \"P0DN37\", \"B3EWG3\", \"Q8TCE6\", \"P15515\", \"Q99879\", \"P02538\", \"P47985\", \"O43423\",\n",
    "    \"Q6PEY2\", \"Q3KPI0\", \"Q9BXR5\", \"Q96JQ5\", \"Q8N138\"\n",
    "]\n",
    "\n",
    "# Load the protein-level results\n",
    "protein_level_file = 'example_protein_reanalysis.tsv'\n",
    "protein_df = pd.read_csv(protein_level_file, sep='\\t')\n",
    "\n",
    "# Filter for proteins in the provided list\n",
    "filtered_protein_df = protein_df[protein_df['Protein'].isin(protein_list)]\n",
    "\n",
    "# Save the filtered dataframe\n",
    "filtered_protein_df.to_csv('filtered_example_protein_reanalysis.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Optionally display the filtered dataframe\n",
    "filtered_protein_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
