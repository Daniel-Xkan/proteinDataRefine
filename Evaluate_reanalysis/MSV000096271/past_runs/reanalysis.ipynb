{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSGF VS PeptideAtlas Reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reqired files:\n",
    "###### 1 all_usi.txt : all peptide atlas usi found\n",
    "###### 2 MassIVE-KB_HPP_proteins.tsv: all MassIVE proteins\n",
    "###### 3 all ambituity files on same directory eg. MSV000096271\n",
    "###### 4 fasta_file = 'uniprotkb_human_proteome_UP000005640_with_isoforms_2024-10-08.fasta'\n",
    "###### 5 PA_observations.csv peptide atlas all peptides\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge ambiguity files --> ambiguity_merged.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the TSV files\n",
    "directory = 'c:/Users/xuech/Desktop/UCSD/grad/ccmc/proteinDataRefine/Evaluate_reanalysis/MSV000096271/'\n",
    "\n",
    "# Output file path\n",
    "output_file = os.path.join(directory, 'ambiguity_merged.tsv')\n",
    "\n",
    "# Archive directory\n",
    "archive_directory = os.path.join(directory, 'ambiguity_archive')\n",
    "\n",
    "# Create the archive directory if it doesn't exist\n",
    "if not os.path.exists(archive_directory):\n",
    "    os.makedirs(archive_directory)\n",
    "\n",
    "# Remove the output file if it already exists\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('MSGF-PLUS-AMBIGUITY') and filename.endswith('.tsv'):\n",
    "        print(f'Processing file: {filename}')\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, sep='\\t', low_memory=False)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f'Skipping empty file: {filename}')\n",
    "            continue\n",
    "        # Append to the output file\n",
    "        df.to_csv(output_file, sep='\\t', index=False, mode='a', header=not os.path.exists(output_file))\n",
    "        # Move the processed file to the archive directory\n",
    "        shutil.move(filepath, os.path.join(archive_directory, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ambiguity_merged.tsv --> filtered.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets in chunks\n",
    "chunksize = 10000\n",
    "proteins_to_skip_file = 'MassIVE-KB_HPP_proteins.tsv'\n",
    "with open(proteins_to_skip_file, 'r') as file:\n",
    "    massive_kb_proteins = set(line.strip() for line in file)\n",
    "\n",
    "filtered_chunks = []\n",
    "for chunk in pd.read_csv('ambiguity_merged.tsv', sep='\\t', low_memory=False, error_bad_lines=False, chunksize=chunksize):\n",
    "    # Process the 'opt_global_TopCanonicalProtein' column to extract the protein ID\n",
    "    chunk['opt_global_TopCanonicalProtein'] = chunk['opt_global_TopCanonicalProtein'].apply(\n",
    "        lambda x: x.split('|')[1] if isinstance(x, str) and '|' in x else x\n",
    "    )\n",
    "    # Filter the chunk\n",
    "    filtered_chunk = chunk[~chunk['opt_global_TopCanonicalProtein'].isin(massive_kb_proteins)]\n",
    "    filtered_chunks.append(filtered_chunk)\n",
    "\n",
    "# Concatenate all filtered chunks\n",
    "filtered_reanalysis_df = pd.concat(filtered_chunks)\n",
    "\n",
    "# Save the filtered dataframe to a new file\n",
    "filtered_reanalysis_df.to_csv('filtered.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spectrum level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all_usi.txt + filtered.tsv --> example_reanalysis_spectrum.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 578 spectrum files and 5381 scans.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the PeptideAtlas USIs from merged.txt\n",
    "usi_file = 'all_usi.txt'\n",
    "usi_data = []\n",
    "with open(usi_file, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(':')\n",
    "        if len(parts) == 6:\n",
    "            dataset, spectrum_file, scan, peptide_identification, peptide_charge = parts[1], parts[2], parts[4], parts[5].split('/')[0], parts[5].split('/')[1]\n",
    "            usi_data.append([line.strip(), dataset, spectrum_file, scan, peptide_identification, peptide_charge])\n",
    "\n",
    "usi_df = pd.DataFrame(usi_data, columns=['USI', 'Dataset', 'Spectrum_File', 'Scan_Number', 'Peptide_Identification', 'Peptide_Charge'])\n",
    "usi_df.to_csv('df_print.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Create a dictionary for quick lookup\n",
    "usi_dict = {}\n",
    "for _, row in usi_df.iterrows():\n",
    "    key = (row['Spectrum_File'], row['Scan_Number'])\n",
    "    usi_dict[key] = row\n",
    "\n",
    "# Read the reanalysis results from MSGF-PLUS-AMBIGUITY-81a33a88-group_by_spectrum-main.tsv\n",
    "reanalysis_file = 'filtered.tsv'\n",
    "reanalysis_df = pd.read_csv(reanalysis_file, sep='\\t', low_memory=False)\n",
    "\n",
    "# Initialize columns for the output DataFrame\n",
    "reanalysis_df.insert(0, 'PeptideAtlas_USI', '')\n",
    "reanalysis_df.insert(1, 'PeptideAtlas_peptide', '')\n",
    "reanalysis_df.insert(2, 'PeptideAtlas_peptide_demod', '')\n",
    "reanalysis_df.insert(3, 'Peptide_match', '')\n",
    "reanalysis_df.insert(4, 'PeptideAtlas_charge', '')\n",
    "\n",
    "\n",
    "# Match spectra from the PeptideAtlas USIs lists to the reanalysis results\n",
    "for index, row in reanalysis_df.iterrows():\n",
    "    original_filepath = row['opt_global_OriginalFilepath']\n",
    "    scan_number = str(row['opt_global_scan'])\n",
    "    \n",
    "    # Extract the spectrum file name from the original file path and remove the .mzML extension if present\n",
    "    spectrum_file = original_filepath.split('/')[-1].replace('.mzML', '')\n",
    "    \n",
    "    key = (spectrum_file, scan_number)\n",
    "    if key in usi_dict:\n",
    "        usi_row = usi_dict[key]\n",
    "        reanalysis_df.at[index, 'PeptideAtlas_USI'] = usi_row['USI']\n",
    "        reanalysis_df.at[index, 'PeptideAtlas_peptide'] = usi_row['Peptide_Identification']\n",
    "        # Remove all substrings like \"[*]\" from PeptideAtlas_peptide\n",
    "        peptide_demod = re.sub(r'\\[.*?\\]', '', usi_row['Peptide_Identification'])\n",
    "        reanalysis_df.at[index, 'PeptideAtlas_peptide_demod'] = peptide_demod\n",
    "        \n",
    "        # Set Peptide_match to 1 if PeptideAtlas_peptide_demod matches opt_global_UnmodPep, otherwise 0\n",
    "        reanalysis_df.at[index, 'Peptide_match'] = 1 if peptide_demod == row['opt_global_UnmodPep'] else 0\n",
    "        reanalysis_df.at[index, 'PeptideAtlas_charge'] = usi_row['Peptide_Charge']\n",
    "        #print(f'Matched file: {usi_row[\"Spectrum_File\"]}, Scan number: {usi_row[\"Scan_Number\"]}')\n",
    "    \n",
    "    # if index % 100 == 0:\n",
    "    #     print(f'Processed {index} rows.')\n",
    "\n",
    "    # Identify the datasets and spectrum files that have at least one match\n",
    "    # matched_datasets = set()\n",
    "matched_spectrum_files = set()\n",
    "matched_scans = set()\n",
    "\n",
    "for index, row in reanalysis_df.iterrows():\n",
    "    if row['PeptideAtlas_USI']:\n",
    "        usi_parts = row['PeptideAtlas_USI'].split(':')\n",
    "        if len(usi_parts) == 6:\n",
    "            dataset = usi_parts[1].replace('.mzML', '')\n",
    "            spectrum_file = usi_parts[2]\n",
    "            matched_spectrum_files.add(spectrum_file)\n",
    "            matched_scans.add((spectrum_file, usi_parts[4]))\n",
    "print(f'Matched {len(matched_spectrum_files)} spectrum files and {len(matched_scans)} scans.')\n",
    "\n",
    "# Add empty USIs for unmatched spectra all at once\n",
    "new_rows = []\n",
    "for key, usi_row in usi_dict.items():\n",
    "    spectrum_file, scan_number = key\n",
    "    if usi_row['Dataset'] == 'PXD019643' and spectrum_file in matched_spectrum_files and key not in matched_scans:\n",
    "        new_rows.append({\n",
    "            'PeptideAtlas_USI': usi_row['USI'],\n",
    "            'PeptideAtlas_peptide': usi_row['Peptide_Identification'],\n",
    "            'PeptideAtlas_peptide_demod': re.sub(r'\\[.*?\\]', '', usi_row['Peptide_Identification']),\n",
    "            'Peptide_match': 0,\n",
    "            'PeptideAtlas_charge': usi_row['Peptide_Charge'],\n",
    "            'opt_global_OriginalFilepath': '',\n",
    "            'opt_global_scan': '',\n",
    "            'opt_global_UnmodPep': ''\n",
    "        })\n",
    "\n",
    "# Append all new rows to the DataFrame at once\n",
    "if new_rows:\n",
    "    reanalysis_df = pd.concat([reanalysis_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "\n",
    "output_file = 'example_reanalysis_spectrum.tsv'\n",
    "reanalysis_df.to_csv(output_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peptide Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example_reanalysis_spectrum -> example_reanalysis_peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiprocessing doeson't work here, please directly call compare_reanalyze_peptide.py in terminal or copy and run code below\n",
    "\n",
    "# import pandas as pd\n",
    "# from Bio import SeqIO\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "# import os\n",
    "\n",
    "# # Load the data\n",
    "# chunk_size = 10000\n",
    "# fasta_file = 'uniprotkb_human_proteome_UP000005640_with_isoforms_2024-10-08.fasta'\n",
    "\n",
    "# # Define a function to get the peptide sequence\n",
    "# def get_peptide_sequence(row):\n",
    "#     return row['PeptideAtlas_peptide_demod'] if pd.notna(row['PeptideAtlas_peptide_demod']) else row['opt_global_UnmodPep']\n",
    "\n",
    "# # Define a function to get the peptide charge\n",
    "# def get_peptide_charge(row):\n",
    "#     return row['PeptideAtlas_charge'] if pd.notna(row['PeptideAtlas_charge']) else row['charge']\n",
    "\n",
    "# # Define a function to get the protein ID from the sequence column \"tr|D9J307|D9J307_HUMAN\"\n",
    "# def get_protein_id_from_msgf(row):\n",
    "#     accession = row['accession']\n",
    "#     if isinstance(accession, str) and '|' in accession:\n",
    "#         parts = accession.split('|')\n",
    "#         if len(parts) > 1:\n",
    "#             return parts[1]\n",
    "#     return None\n",
    "\n",
    "# # Load protein sequences from fasta file\n",
    "# # Pre-compute and store sequences in a dictionary for faster lookups\n",
    "# protein_sequences = {}\n",
    "# for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "#     gene_id = next((part.split('=')[1] for part in record.description.split() if part.startswith('GN=')), 'UNKNOWN')\n",
    "#     protein_sequences[record.id] = {\n",
    "#         'gene_id': gene_id,\n",
    "#         'sequence': str(record.seq).replace('I', 'L')\n",
    "#     }\n",
    "\n",
    "# # Function to count matches in protein sequences\n",
    "# def count_matches(peptide, allow_mutation=False):\n",
    "#     peptide = peptide.replace('I', 'L')\n",
    "#     protein_ids = set()\n",
    "#     gene_ids = set()\n",
    "#     protein_list = []\n",
    "#     gene_list = []\n",
    "    \n",
    "#     for header, data in protein_sequences.items():\n",
    "#         gene_id = data['gene_id']\n",
    "#         sequence = data['sequence']\n",
    "#         protein_id = header.split('|')[1] if '|' in header else header\n",
    "        \n",
    "#         if allow_mutation:\n",
    "#             # Check for near-matches (SAAP)\n",
    "#             for i in range(len(sequence) - len(peptide) + 1):\n",
    "#                 window = sequence[i:i+len(peptide)]\n",
    "#                 if sum(1 for a, b in zip(peptide, window) if a != b) <= 1:\n",
    "#                     protein_ids.add(protein_id)\n",
    "#                     gene_ids.add(gene_id)\n",
    "#                     break\n",
    "#         else:\n",
    "#             # Check for exact matches\n",
    "#             if peptide in sequence:\n",
    "#                 protein_ids.add(protein_id)\n",
    "#                 gene_ids.add(gene_id)\n",
    "    \n",
    "#     # Return counts and lists\n",
    "#     return (\n",
    "#         len(protein_ids), \n",
    "#         len(gene_ids), \n",
    "#         ';'.join(protein_ids) if protein_ids else 'None', \n",
    "#         ';'.join(gene_ids) if gene_ids else 'UNKNOWN'\n",
    "#     )\n",
    "\n",
    "# # Function to process a peptide\n",
    "# def process_peptide(to_parallel_process):\n",
    "#     peptide, peptide_data, peptideatlas_df = to_parallel_process\n",
    "    \n",
    "#     # Get exact matches and SAAP matches\n",
    "#     num_proteins, num_genes, list_proteins, list_genes = count_matches(peptide)\n",
    "#     num_proteins_saap, num_genes_saap, list_proteins_saap, list_genes_saap = count_matches(peptide, allow_mutation=True)\n",
    "    \n",
    "#     # Build the output row\n",
    "#     peptide_row = {\n",
    "#         'Peptide sequence': peptide,\n",
    "#         'Peptide charge': peptide_data.apply(get_peptide_charge, axis=1).iloc[0],\n",
    "#         'Protein identifier': peptide_data.apply(get_protein_id_from_msgf, axis=1).iloc[0],\n",
    "#         'Num_specs_both': len(peptide_data[(pd.notna(peptide_data['PeptideAtlas_USI'])) & (pd.notna(peptide_data['sequence']))]),\n",
    "#         'Num_specs_MSGF': len(peptide_data[(pd.isna(peptide_data['PeptideAtlas_USI'])) & (pd.notna(peptide_data['sequence']))]),\n",
    "#         'Num_specs_PA': len(peptide_data[(pd.notna(peptide_data['PeptideAtlas_USI'])) & (pd.isna(peptide_data['sequence']))]),\n",
    "#         'PA_peptide': 1 if not peptideatlas_df[peptideatlas_df.iloc[:, 5] == peptide].empty else 0,\n",
    "#         'PA_psms': len(peptideatlas_df[peptideatlas_df.iloc[:, 5] == peptide]),\n",
    "#         'Num_proteins': num_proteins,\n",
    "#         'List_proteins': list_proteins,\n",
    "#         'Num_genes': num_genes,\n",
    "#         'List_genes': list_genes,\n",
    "#         'Num_proteins_saap': num_proteins_saap,\n",
    "#         'List_proteins_saap': list_proteins_saap,\n",
    "#         'Num_genes_saap': num_genes_saap,\n",
    "#         'List_genes_saap': list_genes_saap\n",
    "#     }\n",
    "#     #print(peptide_row)\n",
    "#     return peptide_row\n",
    "#     # Function to process an existing peptide\n",
    "# def process_existing_peptide(peptide, peptide_data, peptideatlas_df):\n",
    "#     # Build the output row without counting matches\n",
    "#     peptide_row = {\n",
    "#         'Peptide sequence': peptide,\n",
    "#         'Peptide charge': peptide_data.apply(get_peptide_charge, axis=1).iloc[0],\n",
    "#         'Protein identifier': peptide_data.apply(get_protein_id_from_msgf, axis=1).iloc[0],\n",
    "#         'Num_specs_both': len(peptide_data[(pd.notna(peptide_data['PeptideAtlas_USI'])) & (pd.notna(peptide_data['sequence']))]),\n",
    "#         'Num_specs_MSGF': len(peptide_data[(pd.isna(peptide_data['PeptideAtlas_USI'])) & (pd.notna(peptide_data['sequence']))]),\n",
    "#         'Num_specs_PA': len(peptide_data[(pd.notna(peptide_data['PeptideAtlas_USI'])) & (pd.isna(peptide_data['sequence']))]),\n",
    "#         'PA_peptide': 1 if not peptideatlas_df[peptideatlas_df.iloc[:, 5] == peptide].empty else 0,\n",
    "#         'PA_psms': len(peptideatlas_df[peptideatlas_df.iloc[:, 5] == peptide]),\n",
    "#         'Num_proteins': None,\n",
    "#         'List_proteins': None,\n",
    "#         'Num_genes': None,\n",
    "#         'List_genes': None,\n",
    "#         'Num_proteins_saap': None,\n",
    "#         'List_proteins_saap': None,\n",
    "#         'Num_genes_saap': None,\n",
    "#         'List_genes_saap': None\n",
    "#     }\n",
    "#     print(f\"Existing Peptide ID: {peptide_row['Peptide sequence']}\")\n",
    "#     return peptide_row\n",
    "# # Load the PeptideAtlas data\n",
    "# peptideatlas_df = pd.read_csv('PeptideAtlas_peptides.tsv', sep='\\t')\n",
    "\n",
    "# # Use a set to keep track of unique peptides\n",
    "# unique_peptides_set = set()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Open the output file\n",
    "#     peptide_file_exists = os.path.exists('example_reanalysis_peptide.tsv')\n",
    "#     with open('example_reanalysis_peptide.tsv', 'a+') as output_file:\n",
    "#         # Write the header\n",
    "#         if not peptide_file_exists:\n",
    "#             output_file.write('\\t'.join([\n",
    "#                 'Peptide sequence', 'Peptide charge', 'Protein identifier', 'Num_specs_both', 'Num_specs_MSGF', 'Num_specs_PA',\n",
    "#                 'PA_peptide', 'PA_psms', 'Num_proteins', 'List_proteins', 'Num_genes', 'List_genes', 'Num_proteins_saap',\n",
    "#                 'List_proteins_saap', 'Num_genes_saap', 'List_genes_saap'\n",
    "#             ]) + '\\n')\n",
    "        \n",
    "#         # Read the spectrum file in chunks\n",
    "#         # Check if the file exists\n",
    "#         if peptide_file_exists:\n",
    "#             # Read the existing peptides from the file\n",
    "#             with open('example_reanalysis_peptide.tsv', 'r') as existing_file:\n",
    "#                 # Skip the header\n",
    "#                 next(existing_file)\n",
    "#                 for line in existing_file:\n",
    "#                     peptide = line.split('\\t')[0]  # Extract the peptide sequence (first column)\n",
    "#                     unique_peptides_set.add(peptide)\n",
    "#         print(f\"Number of unique peptides added: {len(unique_peptides_set)}\")\n",
    "#         for chunk in pd.read_csv('example_reanalysis_spectrum.tsv', sep='\\t', chunksize=chunk_size):\n",
    "#             chunk['Peptide sequence'] = chunk.apply(get_peptide_sequence, axis=1)\n",
    "#             counter = 0\n",
    "#             total_peptides = len(chunk['Peptide sequence'].unique())\n",
    "\n",
    "#             def update_counter(result):\n",
    "#                 global counter\n",
    "#                 counter += 1\n",
    "#                 # print(f\"Processed {counter}/{total_peptides} peptides\")\n",
    "\n",
    "#             with Pool(cpu_count()) as pool:\n",
    "#                 peptides_to_process = [\n",
    "#                     (peptide, chunk[chunk['Peptide sequence'] == peptide], peptideatlas_df)\n",
    "#                     for peptide in chunk['Peptide sequence'].unique()\n",
    "#                 ]\n",
    "                \n",
    "#                 results = []\n",
    "#                 to_parallel_process = []\n",
    "#                 not_parallel_process = []\n",
    "\n",
    "#                 for peptide, peptide_data, pa_df in peptides_to_process:\n",
    "#                     if peptide not in unique_peptides_set:\n",
    "#                         #print(f\"Appending New Peptide: {peptide}\")\n",
    "#                         unique_peptides_set.add(peptide)\n",
    "#                         to_parallel_process.append((peptide, peptide_data, pa_df))\n",
    "#                     else:\n",
    "#                         #print(f\"Processing Existing Peptide: {peptide}\")\n",
    "#                         not_parallel_process.append((peptide, peptide_data, pa_df))\n",
    "\n",
    "#                 # Parallel process the new peptides\n",
    "#                 if to_parallel_process:\n",
    "#                     for result in pool.imap_unordered(process_peptide, to_parallel_process):\n",
    "#                         results.append(result)\n",
    "#                         update_counter(result)\n",
    "#                         print(f\"Processed {counter}/{total_peptides} peptides in chunk {chunk.index[0] // chunk_size + 1}\")\n",
    "\n",
    "#                 # Process the existing peptides\n",
    "#                 for peptide, peptide_data, pa_df in not_parallel_process:\n",
    "#                     results.append(process_existing_peptide(peptide, peptide_data, pa_df))\n",
    "#                     update_counter(None)\n",
    "#                     print(f\"Processed {counter}/{total_peptides} peptides in chunk {chunk.index[0] // chunk_size + 1}\")\n",
    "                    \n",
    "#                 for peptide_row in results:\n",
    "#                     peptide_sequence = peptide_row['Peptide sequence']\n",
    "                    \n",
    "#                     # Read the current output file content\n",
    "#                     output_file.seek(0)\n",
    "#                     lines = output_file.readlines()\n",
    "#                     print()\n",
    "#                     # Check if the peptide is already in the file\n",
    "#                     found = False\n",
    "#                     for i, line in enumerate(lines):\n",
    "#                         if line.startswith(peptide_sequence):\n",
    "#                             found = True\n",
    "#                             existing_data = line.strip().split('\\t')\n",
    "                            \n",
    "#                             # Update the existing line with new data\n",
    "#                             existing_data[3] = str(int(existing_data[3]) + peptide_row['Num_specs_both'])\n",
    "#                             existing_data[4] = str(int(existing_data[4]) + peptide_row['Num_specs_MSGF'])\n",
    "#                             existing_data[5] = str(int(existing_data[5]) + peptide_row['Num_specs_PA'])\n",
    "#                             existing_data[6] = str(int(existing_data[6]) + peptide_row['PA_peptide'])\n",
    "#                             existing_data[7] = str(int(existing_data[7]) + peptide_row['PA_psms'])\n",
    "                            \n",
    "#                             # Write the updated line back to the file\n",
    "#                             lines[i] = '\\t'.join(existing_data) + '\\n'\n",
    "#                             break\n",
    "                    \n",
    "#                     if not found:\n",
    "#                         # Append the new peptide row to the file\n",
    "#                         lines.append('\\t'.join(map(str, peptide_row.values())) + '\\n')\n",
    "                    \n",
    "#                     # Write the updated content back to the file\n",
    "#                     output_file.seek(0)\n",
    "#                     output_file.truncate()\n",
    "#                     output_file.writelines(lines)\n",
    "\n",
    "#                 pool.close()\n",
    "#                 pool.join()\n",
    "\n",
    "#             print(f'Processed chunk {chunk.index[0] // chunk_size + 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fix peptide number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuech\\AppData\\Local\\Temp\\ipykernel_9020\\3059300447.py:10: DtypeWarning: Columns (6,8,13,18,23,25,26,27,31,42,48,51,52,54,55,60,61,64,65,69,78,83,85,89,90,91,92,93,94,95,96,97,101,112,118,121,122,124,125,130,131,134,135,139,148,153,155,159,160,164,165,166) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  spectrum_df = pd.read_csv(spectrum_data_path, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing peptide 1/6538 (0.0%)\n",
      "Processing peptide 101/6538 (1.5%)\n",
      "Processing peptide 201/6538 (3.1%)\n",
      "Processing peptide 301/6538 (4.6%)\n",
      "Processing peptide 401/6538 (6.1%)\n",
      "Processing peptide 501/6538 (7.7%)\n",
      "Processing peptide 601/6538 (9.2%)\n",
      "Processing peptide 701/6538 (10.7%)\n",
      "Processing peptide 801/6538 (12.3%)\n",
      "Processing peptide 901/6538 (13.8%)\n",
      "Processing peptide 1001/6538 (15.3%)\n",
      "Processing peptide 1101/6538 (16.8%)\n",
      "Processing peptide 1201/6538 (18.4%)\n",
      "Processing peptide 1301/6538 (19.9%)\n",
      "Processing peptide 1401/6538 (21.4%)\n",
      "Processing peptide 1501/6538 (23.0%)\n",
      "Processing peptide 1601/6538 (24.5%)\n",
      "Processing peptide 1701/6538 (26.0%)\n",
      "Processing peptide 1801/6538 (27.5%)\n",
      "Processing peptide 1901/6538 (29.1%)\n",
      "Processing peptide 2001/6538 (30.6%)\n",
      "Processing peptide 2101/6538 (32.1%)\n",
      "Processing peptide 2201/6538 (33.7%)\n",
      "Processing peptide 2301/6538 (35.2%)\n",
      "Processing peptide 2401/6538 (36.7%)\n",
      "Processing peptide 2501/6538 (38.3%)\n",
      "Processing peptide 2601/6538 (39.8%)\n",
      "Processing peptide 2701/6538 (41.3%)\n",
      "Processing peptide 2801/6538 (42.8%)\n",
      "Processing peptide 2901/6538 (44.4%)\n",
      "Processing peptide 3001/6538 (45.9%)\n",
      "Processing peptide 3101/6538 (47.4%)\n",
      "Processing peptide 3201/6538 (49.0%)\n",
      "Processing peptide 3301/6538 (50.5%)\n",
      "Processing peptide 3401/6538 (52.0%)\n",
      "Processing peptide 3501/6538 (53.5%)\n",
      "Processing peptide 3601/6538 (55.1%)\n",
      "Processing peptide 3701/6538 (56.6%)\n",
      "Processing peptide 3801/6538 (58.1%)\n",
      "Processing peptide 3901/6538 (59.7%)\n",
      "Processing peptide 4001/6538 (61.2%)\n",
      "Processing peptide 4101/6538 (62.7%)\n",
      "Processing peptide 4201/6538 (64.3%)\n",
      "Processing peptide 4301/6538 (65.8%)\n",
      "Processing peptide 4401/6538 (67.3%)\n",
      "Processing peptide 4501/6538 (68.8%)\n",
      "Processing peptide 4601/6538 (70.4%)\n",
      "Processing peptide 4701/6538 (71.9%)\n",
      "Processing peptide 4801/6538 (73.4%)\n",
      "Processing peptide 4901/6538 (75.0%)\n",
      "Processing peptide 5001/6538 (76.5%)\n",
      "Processing peptide 5101/6538 (78.0%)\n",
      "Processing peptide 5201/6538 (79.6%)\n",
      "Processing peptide 5301/6538 (81.1%)\n",
      "Processing peptide 5401/6538 (82.6%)\n",
      "Processing peptide 5501/6538 (84.1%)\n",
      "Processing peptide 5601/6538 (85.7%)\n",
      "Processing peptide 5701/6538 (87.2%)\n",
      "Processing peptide 5801/6538 (88.7%)\n",
      "Processing peptide 5901/6538 (90.3%)\n",
      "Processing peptide 6001/6538 (91.8%)\n",
      "Processing peptide 6101/6538 (93.3%)\n",
      "Processing peptide 6201/6538 (94.8%)\n",
      "Processing peptide 6301/6538 (96.4%)\n",
      "Processing peptide 6401/6538 (97.9%)\n",
      "Processing peptide 6501/6538 (99.4%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the peptide and spectrum data\n",
    "peptide_data_path = \"example_reanalysis_peptide.tsv\"\n",
    "spectrum_data_path = \"example_reanalysis_spectrum.tsv\"\n",
    "peptideatlas_df = pd.read_csv('PeptideAtlas_peptides.tsv', sep=\"\\t\")\n",
    "\n",
    "peptide_df = pd.read_csv(peptide_data_path, sep=\"\\t\")\n",
    "spectrum_df = pd.read_csv(spectrum_data_path, sep=\"\\t\")\n",
    "\n",
    "\n",
    "def get_protein_id_from_pa(row):\n",
    "    peptide_sequence = row['Peptide sequence']\n",
    "    matching_rows = peptideatlas_df[peptideatlas_df.iloc[:, 5] == peptide_sequence]\n",
    "    if not matching_rows.empty:\n",
    "        return matching_rows.iloc[0, 0]  # Return the protein ID from column 0\n",
    "    return None\n",
    "output_df = pd.DataFrame()\n",
    "\n",
    "# Function to update the specified columns\n",
    "def update_peptide_data(peptide_df, spectrum_df):\n",
    "    # Iterate through each peptide in the peptide dataframe\n",
    "    pa_observations_df = pd.read_csv('PA_observations.csv')\n",
    "    for index, row in peptide_df.iterrows():\n",
    "        # Print progress update\n",
    "        if index % 100 == 0:\n",
    "            print(f\"Processing peptide {index+1}/{len(peptide_df)} ({(index+1)/len(peptide_df)*100:.1f}%)\")\n",
    "        peptide_sequence = row['Peptide sequence']\n",
    "        \n",
    "        # Filter spectrum data for the current peptide\n",
    "        spectrum_subset = spectrum_df[(spectrum_df['sequence'] == peptide_sequence) | (spectrum_df['PeptideAtlas_peptide_demod'] == peptide_sequence)]\n",
    "        \n",
    "        # Update the columns\n",
    "        num_specs_both = len(spectrum_subset[(pd.notna(spectrum_subset['PeptideAtlas_USI'])) & (pd.notna(spectrum_subset['sequence']))])\n",
    "        num_specs_msgf = len(spectrum_subset[(pd.isna(spectrum_subset['PeptideAtlas_USI'])) & (pd.notna(spectrum_subset['sequence']))])\n",
    "        num_specs_pa = len(spectrum_subset[(pd.notna(spectrum_subset['PeptideAtlas_USI'])) & (pd.isna(spectrum_subset['sequence']))])\n",
    "        pa_peptide = 0\n",
    "\n",
    "        if peptide_sequence in pa_observations_df['Sequence'].values:\n",
    "            pa_peptide = 1\n",
    "        # Count PSMs from all_usi.txt\n",
    "        pa_psms = 0\n",
    "        # try:\n",
    "        #     with open('all_usii.txt', 'r') as usi_file:\n",
    "        #         for line in usi_file:\n",
    "        #             usi = line.strip()\n",
    "        #             # Extract and demodify the peptide sequence from the USI\n",
    "        #             if \":\" not in usi or \"/\" not in usi:\n",
    "        #                 continue\n",
    "                    \n",
    "        #             # Extract sequence part between last colon and slash\n",
    "        #             parts = usi.split(':')\n",
    "        #             if len(parts) < 2:\n",
    "        #                 continue\n",
    "                    \n",
    "        #             seq_part = parts[-1].split('/')[0]\n",
    "                    \n",
    "        #             # Remove modifications (text in square brackets)\n",
    "        #             demod_seq = re.sub(r'\\[.*?\\]', '', seq_part)\n",
    "                    \n",
    "        #             # If peptide matches the current peptide sequence\n",
    "        #             if demod_seq == peptide_sequence:\n",
    "        #                 pa_psms += 1\n",
    "        # except FileNotFoundError:\n",
    "        # pa_psms = len(spectrum_subset[spectrum_subset['sequence'] == peptide_sequence])\n",
    "        \n",
    "        # Update the peptide dataframe\n",
    "        \n",
    "        protein_identifier = row['Protein identifier']\n",
    "        if isinstance(protein_identifier, str):\n",
    "            peptide_df.at[index, 'Protein identifier MSGF'] = protein_identifier.split('-')[0]\n",
    "        else:\n",
    "            peptide_df.at[index, 'Protein identifier MSGF'] = None\n",
    "        peptide_df.at[index, 'Protein identifier PA'] = \";\".join(peptideatlas_df[peptideatlas_df.iloc[:, 5] == peptide_sequence].iloc[:, 0].dropna().unique())\n",
    "        peptide_df.at[index, 'Num_specs_both'] = num_specs_both\n",
    "        peptide_df.at[index, 'Num_specs_MSGF'] = num_specs_msgf\n",
    "        peptide_df.at[index, 'Num_specs_PA'] = num_specs_pa\n",
    "        peptide_df.at[index, 'PA_peptide'] = pa_peptide\n",
    "        peptide_df.at[index, 'PA_psms'] = pa_psms\n",
    "\n",
    "    return peptide_df\n",
    "\n",
    "# Update the peptide data\n",
    "updated_peptide_df = update_peptide_data(peptide_df, spectrum_df)\n",
    "\n",
    "# Save the updated dataframe\n",
    "updated_peptide_df.to_csv(\"example_peptide_reanalysis_updated.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "peptide_file = 'example_peptide_reanalysis_updated.tsv'\n",
    "protein_file = 'PeptideAtlas_proteins_not_in_MassIVE.tsv'\n",
    "\n",
    "peptide_df = pd.read_csv(peptide_file, sep='\\t')\n",
    "protein_df = pd.read_csv(protein_file, sep='\\t')\n",
    "\n",
    "# Initialize the result dataframe\n",
    "result_columns = [\n",
    "    'Protein', 'Num_peptides_both', 'Num_peptides_MSGF', 'Num_peptides_PA',\n",
    "    'Num_unique_both', 'Num_unique_MSGF', 'Num_unique_PA',\n",
    "    'Num_specs_both', 'Num_specs_MSGF', 'Num_specs_PA'\n",
    "]\n",
    "result_df = pd.DataFrame(columns=result_columns)\n",
    "\n",
    "# Iterate over each protein\n",
    "result_list = []\n",
    "for protein in protein_df['nextprot_accession']:\n",
    "    # Filter peptides that match the current protein\n",
    "    matched_peptides = peptide_df[\n",
    "        (peptide_df['Protein identifier MSGF'] == protein) | \n",
    "        (peptide_df['Protein identifier PA'] == protein)\n",
    "    ]\n",
    "    \n",
    "    # Calculate the required counts and sums\n",
    "    num_peptides_both = len(matched_peptides[matched_peptides['Num_specs_both'] > 0])\n",
    "    num_peptides_MSGF = len(matched_peptides[matched_peptides['Num_specs_MSGF'] > 0])\n",
    "    num_peptides_PA = len(matched_peptides[matched_peptides['Num_specs_PA'] > 0])\n",
    "    \n",
    "    num_unique_both = len(matched_peptides[(matched_peptides['Num_specs_both'] > 0) & (matched_peptides['Num_genes_saap'] == 1)])\n",
    "    num_unique_MSGF = len(matched_peptides[(matched_peptides['Num_specs_MSGF'] > 0) & (matched_peptides['Num_genes_saap'] == 1)])\n",
    "    num_unique_PA = len(matched_peptides[(matched_peptides['Num_specs_PA'] > 0) & (matched_peptides['Num_genes_saap'] == 1)])\n",
    "    \n",
    "    num_specs_both = matched_peptides['Num_specs_both'].sum()\n",
    "    num_specs_MSGF = matched_peptides['Num_specs_MSGF'].sum()\n",
    "    num_specs_PA = matched_peptides['Num_specs_PA'].sum()\n",
    "    \n",
    "    # Append the results to the result list\n",
    "    result_list.append({\n",
    "        'Protein': protein,\n",
    "        'Num_peptides_both': num_peptides_both,\n",
    "        'Num_peptides_MSGF': num_peptides_MSGF,\n",
    "        'Num_peptides_PA': num_peptides_PA,\n",
    "        'Num_unique_both': num_unique_both,\n",
    "        'Num_unique_MSGF': num_unique_MSGF,\n",
    "        'Num_unique_PA': num_unique_PA,\n",
    "        'Num_specs_both': num_specs_both,\n",
    "        'Num_specs_MSGF': num_specs_MSGF,\n",
    "        'Num_specs_PA': num_specs_PA\n",
    "    })\n",
    "\n",
    "# Convert the result list to a dataframe\n",
    "result_df = pd.concat([pd.DataFrame([result]) for result in result_list], ignore_index=True)\n",
    "\n",
    "# Save the result to a new file in TSV format\n",
    "result_df.to_csv('example_protein_reanalysis.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
